<!DOCTYPE html>
<html lang="en">
<head>
<title>Statistics for Data Science (University of Waterloo Professional Development)</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="shortcut icon" href="../img/favicon.ico">
<link rel="stylesheet" href="../css/bootstrap.min.css">
</head>
<body>
<div class="container">
    <h1 class="text-center mt-5 mb-5">Statistics for Data Science</h1>
    <p class="text-end mb-5"><em>University of Waterloo Professional Development</em></p>
    <div class="accordion" id="accordionExample">
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingOne">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">Introduction to Statistics for Data Science</button>
            </h2>
            <div id="collapseOne" class="accordion-collapse collapse" aria-labelledby="headingOne" data-bs-parent="#accordionExample">
                <div class="accordion-body">
                    <p><a href="../docs/timeline-of-statistics.pdf" download>Timeline of statistics, PDF</a></p>
                    <p><span class="badge rounded-pill bg-secondary">Statistics</span> is the study of how best to collect, analyze, and draw conclusions from data (identify the problem &#8594; collect relevant data &#8594; analyze the data &#8594; form a conclusion).</p>
                    <p>A <span class="badge rounded-pill bg-secondary">summary statistic</span> is a single number summarizing a large amount of data. For instance, the primary results of the study after 1 year could be described by two summary statistics: the proportion of people who had a stroke in the treatment (45/224 = 0.20 = 20%) and control (28/227 = 0.12 = 12%) groups.</p>
                    <p>When recording data, use a <span class="badge rounded-pill bg-secondary">data matrix</span> unless you have a very good reason to use a diferent structure. This structure allows new <span class="badge rounded-pill bg-secondary">cases/observational units</span> to be added as rows or new <span class="badge rounded-pill bg-secondary">variables</span> as new columns:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-11.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Types of data:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Quantitative</span> (or <span class="badge rounded-pill bg-secondary">numerical</span>) data deals with numbers and things you can measure objectively (dimensions such as height, width, and length; temperature and humidity; prices; area and volume).
                            <ul>
                                <li><span class="badge rounded-pill bg-secondary">Discrete</span> data is a count of something that cannot be made more precise. Typically, it involves integers (the number of children/adults/pets in your family; measured quantities; results of experiments; numerical values obtained by counting).</li>
                                <li><span class="badge rounded-pill bg-secondary">Continuous</span> data can be divided and reduced to finer and finer levels (you can measure someone's height at progressively more precise scales - meters, centimeters, millimeters, and beyond; values obtained by measuring - e.g. height of all students; all values in a given interval of numbers - e.g. federal spending).</li>
                            </ul>
                        </li>
                        <li><span class="badge rounded-pill bg-secondary">Qualitative</span> (or <span class="badge rounded-pill bg-secondary">categorical</span>) data deals with characteristics and descriptors that cannot be easily measured, but can be observed subjectively (personal tastes; textures; attractiveness; colour).
                            <ul>
                                <li><span class="badge rounded-pill bg-secondary">Ordinal</span> data: When items are assigned to categories that have some kind of implicit or natural order, such as "short, medium, or tall", the data is of ordinal nature. Another example is a survey question that asks us to rate an item on a 1 to 10 scale, with 10 being the best. This implies that 10 is better than 9, which is better than 8, and so on.</li>
                                <li><span class="badge rounded-pill bg-secondary">Nominal</span> data: Any categorical data that doesn't have an order (e.g. "blue", "red", "green").</li>
                                <li><span class="badge rounded-pill bg-secondary">Binary</span> data place things in one of two mutually exclusive categories: right/wrong, true/false, or accept/reject. It is nominal data but with only two distinct categories.</li>
                            </ul>
                        </li>
                        <li><span class="badge rounded-pill bg-secondary">Other</span> non-numerical data such as text or video data.</li>
                    </ul>
                    <p>Broadly speaking, when you measure something and give it a number value, you create <span class="badge rounded-pill bg-secondary">quantitative</span> data. When you classify or judge something, you create <span class="badge rounded-pill bg-secondary">qualitative</span> data.</p>
                    <p>Relationships between data:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Independent variables</span>: If two variables are not associated, then they are said to be independent. That is, two variables are independent if there is no evident relationship between the two.</li>
                        <li><span class="badge rounded-pill bg-secondary">Associated, or dependent</span>: When two variables show some connection with one another, they are called associated variables. Associated variables can also be called dependent variables and vice-versa.</li>
                        <li><span class="badge rounded-pill bg-secondary">Positive association</span>: Two variables are said to be positively associated when they have a linear relationship with a positive slope. That means when the value of one variables increases, the value of the other variable increases as well. For example, the amount people spend is positively associated with the money people make, if we assume that people who earn more spend more.</li>
                        <li><span class="badge rounded-pill bg-secondary">Negative association</span>: When the value of a variable goes down when the value of another variable goes up. This is often characterized by a downward trend when the variables are plotted.</li>
                    </ul>
                    <p>A <span class="badge rounded-pill bg-secondary">scatterplot</span> provides a case-by-case view of data for two numerical variables:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-1.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>When we suspect one variable might causally affect another, we label the first variable the <span class="badge rounded-pill bg-secondary">explanatory</span> variable and the second the <span class="badge rounded-pill bg-secondary">response</span> variable. Is federal spending, on average, higher or lower in areas with high rates of poverty? If we suspect poverty might affect spending, then poverty is the explanatory variable and federal spending is the response variable in the relationship.</p>
                    <p>Labeling variables as explanatory and response <span class="badge rounded-pill bg-secondary">does not guarantee</span> the relationship between them is causal, even if there is an association (also known as a <span class="badge rounded-pill bg-secondary">correlation</span>) between the two variables. It is also important to note that causation is an <span class="badge rounded-pill bg-secondary">asymmetric</span> relation ("X causes Y" is a different statement than "Y causes X"), whereas correlation is a <span class="badge rounded-pill bg-secondary">symmetric</span> relation ("X is correlated with Y" is an equivalent statement to "Y is correlated with X").</p>
                    <p>In statistics, a <span class="badge rounded-pill bg-secondary">confounding</span> variable is a variable that influences both the dependent variable and independent variable giving rise to a misleading association. Confounding is a causal concept and a confounding variable is also known as a <span class="badge rounded-pill bg-secondary">confounding factor</span>, a <span class="badge rounded-pill bg-secondary">lurking variable</span>, or a <span class="badge rounded-pill bg-secondary">confounder</span>. Suppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Some previous research tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, she is more likely to use sunscreen and more likely to get skin cancer. Sun exposure is what is called a confounding variable, which is a variable that is correlated with both the explanatory and response variables.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-12.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p><span class="badge rounded-pill bg-secondary">Collinearity</span> (or <span class="badge rounded-pill bg-secondary">multicollinearity/ill-conditioning</span>) occurs when independent variables in a regression are so highly correlated that it becomes difficult or impossible to distinguish their individual effects on the dependent variable.</p>
                    <p>Data collection:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Observational studies</span> can provide evidence of a naturally occurring association between variables, but they cannot by themselves show a causal connection. For example, data may be collected via surveys, obtaining records (e.g. medical or company records), or by following a cohort of similar individuals as part of a study. In each of these situations, researchers merely observe the data as it arises. Hence, an observational study is conducted when data is collected in a way that does not directly interfere with how the data arises. Data where no treatment has been explicitly applied (or explicitly withheld) is called <span class="badge rounded-pill bg-secondary">observational data</span>.</li>
                        <li>When the possibility of a causal connection needs to be investigated, an <span class="badge rounded-pill bg-secondary">experiment</span> can be conducted. There are both explanatory and response variables in this case. To check if there really is a causal connection between the explanatory variable and the response, a sample of individuals are identified and split into groups. The individuals in each group are assigned a treatment. In statistics, a <span class="badge rounded-pill bg-secondary">treatment</span> is a generic term which refers to the specifics of how each group is handled for the purposes of an experiment.</li>
                    </ul>
                    <p>Generally, data in observational studies are collected only by monitoring what occurs, while experiments require the primary explanatory variable in a study be assigned for each subject by the researchers. Making causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be unreliable and is not recommended. Thus, observational studies are generally only sufficient to show associations.</p>
                    <div class="row">
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-2.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-3.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-4.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-5.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                    </div>
                    <div class="row">
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-6.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-7.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                    </div>
                    <p>This is an experiment, as the researchers assigned the volunteers to a treatment group (beer or water).</p>
                    <p>Forms of observational studies:</p>
                    <ul>
                        <li>A <span class="badge rounded-pill bg-secondary">prospective study</span> identifes individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of patients over many years to assess the possible infuences of behavior on cancer risk. This prospective study recruits registered nurses and then collects data from them using questionnaires.</li>
                        <li><span class="badge rounded-pill bg-secondary">Retrospective studies</span> collect data after events have taken place, e.g. researchers may review past events in medical records.</li>
                    </ul>
                    <p>Studies where the researchers assign treatments to cases are called <span class="badge rounded-pill bg-secondary">experiments</span>. When this assignment includes randomization, e.g. using a coin flip to decide which treatment a patient receives, it is called a <span class="badge rounded-pill bg-secondary">randomized experiment</span>. Randomized experiments are fundamentally important when trying to show a causal connection between two variables. They are generally built on four principles:</p>
                    <ol>
                        <li><span class="badge rounded-pill bg-secondary">Controlling</span>. Researchers assign treatments to cases, and they do their best to control any other differences in the groups. Suppose a farmer wishes to evaluate a new fertilizer. She uses the new fertilizer on one field of crops (A), while using her current fertilizer on another field of crops (B). The irrigation system on field A has recently been repaired and provides adequate water to all of the crops, while the system on field B will not be repaired until next season. She concludes that the new fertilizer is far superior. The problem with this experiment is that the farmer has neglected to control for the effect of the differences in irrigation. This leads to <span class="badge rounded-pill bg-secondary">experimental bias</span>, the favoring of certain outcomes over others. To avoid this bias, the farmer should have tested the new fertilizer in identical conditions to the control group, which did not receive the treatment.</li>
                        <li><span class="badge rounded-pill bg-secondary">Randomization</span>. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Using randomization is the most reliable method of creating <span class="badge rounded-pill bg-secondary">homogeneous treatment groups</span>, without involving any potential biases.</li>
                        <li><span class="badge rounded-pill bg-secondary">Replication</span>. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a suffciently large sample. Additionally, a group of scientists may replicate an entire study to verify an earlier finding. </li>
                        <li><span class="badge rounded-pill bg-secondary">Blocking</span>. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable into blocks and then randomize cases within each block to the treatment groups. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients in the study into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients.</li>
                    </ol>
                    <p>A <span class="badge rounded-pill bg-secondary">population</span> is a collection of people, items, or events and includes all members of a defined group that we are studying or collecting information on for data driven decisions.</p>
                    <p>A <span class="badge rounded-pill bg-secondary">sample</span> is a small subset or fraction of a population.</p>
                    <p>A <span class="badge rounded-pill bg-secondary">parameter</span> is any summary number, like an average or percentage, that describes the entire population. For example, the population mean <span class="badge rounded-pill bg-secondary">μ</span> and the population proportion <span class="badge rounded-pill bg-secondary">p</span> are two population parameters.</p>
                    <p>A sample is a finite subset selected from the population with the objective of investigating its properties. The number of units in the sample is known as the <span class="badge rounded-pill bg-secondary">sample size</span>. A sample helps us draw conclusions about the full population. There are 2 key properties of a properly selected sample:</p>
                    <ol>
                        <li>The sample is <span class="badge rounded-pill bg-secondary">randomly</span> selected. A random sample is a group or set chosen from a larger population in a random manner that allows for each member of the larger group to have an equal chance of being chosen. What if we picked a sample by hand? It is entirely possible that the sample could be skewed to that person's interests, which may be entirely unintentional. This introduces bias into a sample.</li>
                        <li>The sample is a <span class="badge rounded-pill bg-secondary">representative</span> sample. A representative sample is a group or set chosen from a larger statistical population that adequately replicates the larger group according to whatever characteristic or quality is under study.</li>
                    </ol>
                    <p>However, although random sampling helps minimize bias, there are still ways in which bias can arise:</p>
                    <ul>
                        <li>In cases of surveys where the non-response rate is high - even if people are picked at random - caution must be taken. For instance, if only 30% of the people randomly sampled for a survey actually respond, then it is unclear whether the results are representative of the entire population. This <span class="badge rounded-pill bg-secondary">non-response bias</span> can skew results.</li>
                        <li>In <span class="badge rounded-pill bg-secondary">convenience samples</span>, only individuals easily accessible are included in the sample. For instance, if a political survey is performed by stopping people walking on Bay Street, this will not represent all of the city of Toronto.</li>
                    </ul>
                    <p>Sampling methods:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Simple random sampling</span>: Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league's 30 teams. To take a simple random sample of 120 baseball players and their salaries, we could write the names of that season's several hundreds of players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as "simple random" if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included.<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-8.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></li>
                        <li><span class="badge rounded-pill bg-secondary">Stratified sampling</span>: The population is divided into groups called <span class="badge rounded-pill bg-secondary">strata</span>. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each <span class="badge rounded-pill bg-secondary">stratum</span>. In the baseball salary example, the teams could represent the strata, since some teams have a lot more money. Then we might randomly sample 4 players from each team for a total of 120 players. Stratifed sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. We might get a more stable estimate for the subpopulation in a stratum if the cases are very similar, leading to more precise estimates within each group. When we combine these estimates into a single estimate for the full population, that population estimate will tend to be more precise since each individual group estimate is itself more precise.<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-9.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></li>
                        <li><span class="badge rounded-pill bg-secondary">Cluster sampling</span>: In a cluster sample, we break up the population into many groups, called <span class="badge rounded-pill bg-secondary">clusters</span>. Then we sample a fixed number of clusters and include all observations from each of those clusters in the sample. It is important to note that, unlike with the strata in stratified sampling, the clusters should be microcosms, rather than subsections, of the population. Each cluster should be heterogeneous. Cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don't look very different from one another.<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-13.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></li>
                        <li><span class="badge rounded-pill bg-secondary">Multistage sampling</span>: A multistage sample is like a cluster sample, but rather than keeping all observations in each cluster, we collect a random sample within each selected cluster.<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-14.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></li>
                    </ul>
                    <p>Please note that although useful in certain circumstances, the use of stratified or cluster sampling can still be very subjective and can introduce bias into the sample.</p>
                    <p><span class="badge rounded-pill bg-secondary">Bias</span> is the intentional or unintentional favouring of one group or outcome over other potential groups or outcomes in the population:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Selection bias</span>: The bias that results from an unrepresentative sample.
                            <ul>
                                <li><span class="badge rounded-pill bg-secondary">Undercoverage bias</span>: Occurs when some members of the population are inadequately represented in the sample.</li>
                                <li><span class="badge rounded-pill bg-secondary">Non-response bias</span>: Bias that results when respondents differ in meaningful ways from non-respondents.</li>
                                <li><span class="badge rounded-pill bg-secondary">Voluntary bias</span>: Sample members are self-selected volunteers.</li>
                            </ul>
                        </li>
                        <li><span class="badge rounded-pill bg-secondary">Response bias</span>: The bias that results from problems in the measurement process.
                            <ul>
                                <li><span class="badge rounded-pill bg-secondary">Leading questions</span>: Questions that encourage the expected answer.</li>
                                <li><span class="badge rounded-pill bg-secondary">Social desirability</span>: Responses may be biased toward what the respondents believe is socially desirable.</li>
                            </ul>
                        </li>
                    </ul>
                    <p>Most experiments try to determine whether some type of experimental treatment (or important factor) has a significant effect on an outcome. For example, does zinc help to reduce the length of a cold? Subjects who are chosen to participate in the experiment are typically divided into two groups: a treatment group and a control group. The <span class="badge rounded-pill bg-secondary">treatment group</span> consists of participants who receive the experimental treatment whose effect is being studied (in this case, zinc tablets). The <span class="badge rounded-pill bg-secondary">control group</span> consists of participants who do not receive the experimental treatment being studied. Instead, they get a placebo (a fake treatment - e.g. a sugar pill); a standard, nonexperimental treatment (such as vitamin C, in the zinc study); or no treatment at all, depending on the situation. After the experiment has been performed, the responses of those in the treatment group are compared with the responses from the control group to look for differences that are statistically significant (i.e. unlikely to have occurred just by chance). The study is ideally <span class="badge rounded-pill bg-secondary">double-blind</span> - researchers who interact with the participants and the participants themselves are all unaware of which group they belong to.</p>
                    <p>A <span class="badge rounded-pill bg-secondary">placebo</span> is a fake treatment, such as a sugar pill used in a medical trial. Placebos are given to the control group to account for a psychological phenomenon called the placebo effect, in which patients receiving a fake treatment still report having a response, as if it were the real treatment. By measuring the placebo effect in the control group, you can assess what portion of the reports from the treatment group were due to a real physical effect and what portion were likely due to the placebo effect.</p>
                    <p><span class="badge rounded-pill bg-secondary">Mean</span>: The average of a set of numbers. The mean is a common way to measure the center of a distribution of data. To find the mean number of characters in set of emails, we add up all the character counts and divide by the number of emails.</p>
                    <p><span class="badge rounded-pill bg-secondary">Median</span>: The value in middle of a sorted list. If there are an even number of observations, there will be two values in the middle of list and the median is calculated as the average of these two numbers.</p>
                    <p><span class="badge rounded-pill bg-secondary">Outliers</span> are values that are unusual compared to the rest of the dataset (i.e. especially small or large in numerical value). If we were to find the average salary of 5 employees, whose salaries are $40k, $50k, $45k, $40k and $100k, we compute mean to be sum of salaries divided by 5 which is $55k. However, this isn't the best representation of the group because most of the salaries are between $40k and $50k. The <span class="badge rounded-pill bg-secondary">mean</span> is skewed by the one large salary ($100k). In this situation we would typically use a better measure of central tendency, such as the <span class="badge rounded-pill bg-secondary">median</span>. Examining data for outliers serves many useful purposes, including identifying strong skew in the distribution, identifying possible data collection or data entry errors, and providing insight into interesting properties of the data.</p>
                    <p><span class="badge rounded-pill bg-secondary">Variance</span>: A measure of the variability of the data. Roughly the average squared distance from the mean.</p>
                    <p><span class="badge rounded-pill bg-secondary">Standard deviation</span>: Roughly describes how far away the typical observation is from the mean; the distance is called deviation. Usually about 70% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. The standard deviation is also the square root of the variance.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-10.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Figure 2.9 shows three distributions that look quite different, but all have the same mean, variance, and standard deviation. Using modality, we can distinguish between the first plot (bimodal) and the last two (unimodal). Using skewness, we can distinguish between the last plot (right skewed) and the first two. While a picture, like a histogram, tells a more complete story, we can use modality and shape (symmetry/skew) to characterize basic information about a distribution:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-28.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>A <span class="badge rounded-pill bg-secondary">dot plot</span> is a one-variable scatterplot:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-25.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-26.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Dot plots show the exact value for each observation. This is useful for small data sets, but they can become hard to read with larger samples. Rather than showing the value of each observation, we prefer to think of the value as belonging to a <span class="badge rounded-pill bg-secondary">bin</span>. For example, in the loan50 data set, we created a table of counts for the number of loans with interest rates between 5.0% and 7.5%, then the number of loans with rates between 7.5% and 10.0%, and so on. Observations that fall on the boundary of a bin (e.g. 10.00%) are allocated to the lower bin. <span class="badge rounded-pill bg-secondary">Histograms</span> provide a view of the <span class="badge rounded-pill bg-secondary">data density</span>. Higher bars represent where the data are relatively more common. Histograms are especially convenient for describing the <span class="badge rounded-pill bg-secondary">shape</span> of the data distribution. The chosen <span class="badge rounded-pill bg-secondary">bin width</span> can alter the story the histogram is telling. When data trail off in one direction, the distribution has a <span class="badge rounded-pill bg-secondary">long tail</span>. If a distribution has a long left tail, it is <span class="badge rounded-pill bg-secondary">left skewed</span>. If a distribution has a long right tail, it is <span class="badge rounded-pill bg-secondary">right skewed</span>. In addition to looking at whether a distribution is skewed or <span class="badge rounded-pill bg-secondary">symmetric</span>, histograms can be used to identify modes. A <span class="badge rounded-pill bg-secondary">mode</span> is represented by a prominent peak in the distribution.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-27.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-15.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-16.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Unimodal and right skewed, with a potentially unusual observation at 60 hours/week.</p>
                    <p>A <span class="badge rounded-pill bg-secondary">box plot</span> summarizes a data set using five statistics while also plotting unusual observations. Figure 2.10 provides a vertical dot plot alongside a box plot of the interest rate variable from the loan50 data set:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-17.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>The most common definition of a <span class="badge rounded-pill bg-secondary">percentile</span> is a number where a certain percentage of scores fall below that number. You might know that you scored 67 out of 90 on a test. But that figure has no real meaning unless you know what percentile you fall into. If you know that your score is in the 90th percentile, that means you scored better than 90% of people who took the test.</p>
                    <p>The 25th percentile is also called the <span class="badge rounded-pill bg-secondary">first quartile</span>, Q1. The 50th percentile is also called the <span class="badge rounded-pill bg-secondary">median</span>. The 75th percentile is also called the <span class="badge rounded-pill bg-secondary">third quartile</span>, Q3. Between Q1 and Q3 is the middle 50% of the data. The range these data span is called the <span class="badge rounded-pill bg-secondary">interquartile range</span>, or the IQR (IQR = Q3 - Q1).</p>
                    <p>The box in a <span class="badge rounded-pill bg-secondary">box plot</span> represents the middle 50% of the data, and the thick line in the box is the median. Whiskers of a box plot can extend up to 1.5 x IQR away from the quartiles: max upper whisker reach = Q3 + 1.5 x IQR, max lower whisker reach = Q1 - 1.5 x IQR (IQR: 20 - 10 = 10, max upper whisker reach = 20 + 1.5 x 10 = 35, max lower whisker reach = 10 - 1.5 x 10 = -5). A potential outlier is defined as an observation beyond the maximum reach of the whiskers. It is an observation that appears extreme relative to the rest of the data.</p>
                    <p>For skewed distributions it is often more helpful to use <span class="badge rounded-pill bg-secondary">median</span> and <span class="badge rounded-pill bg-secondary">IQR</span> to describe the center and spread; for symmetric distributions it is often more helpful to use the <span class="badge rounded-pill bg-secondary">mean</span> and <span class="badge rounded-pill bg-secondary">SD</span> to describe the center and spread. If the distribution is symmetric, center is often defined as the mean (mean ~ median), if the distribution is skewed or has extreme outliers, center is often defined as the median (<span class="badge rounded-pill bg-secondary">right-skewed</span>: mean &gt; median, <span class="badge rounded-pill bg-secondary">left-skewed</span>: mean &lt; median). The median and IQR are called <span class="badge rounded-pill bg-secondary">robust statistics</span> because extreme observations have little effect on their values: moving the most extreme value generally has little influence on these statistics. On the other hand, the mean and standard deviation are more heavily influenced by changes in extreme observations, which can be important in some situations.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-18.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>A table that summarizes data for two categorical variables is called a <span class="badge rounded-pill bg-secondary">contingency table</span>. Each value in the table represents the number of times a particular combination of variable outcomes occurred.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-19.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>A <span class="badge rounded-pill bg-secondary">bar plot</span> is a common way to display a single categorical variable. A bar plot where proportions instead of frequencies are shown is called a <span class="badge rounded-pill bg-secondary">relative frequency bar plot</span>.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-20.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p><span class="badge rounded-pill bg-secondary">Bar plots</span> are used for displaying distributions of categorical variables, while <span class="badge rounded-pill bg-secondary">histograms</span> are used for numerical variables. The x-axis in a histogram is a number line, hence the order of the bars cannot be changed, while in a bar plot the categories can be listed in any order (though some orderings make more sense than others, especially for ordinal variables).</p>
                    <p><span class="badge rounded-pill bg-secondary">Stacked bar plot</span> is a graphical display of contingency table information, for counts. <span class="badge rounded-pill bg-secondary">Side-by-side bar plot</span> displays the same information by placing bars next to, instead of on top of, each other. <span class="badge rounded-pill bg-secondary">Standardized stacked bar plot</span> is a graphical display of contingency table information, for proportions.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-29.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-30.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>A <span class="badge rounded-pill bg-secondary">mosaic plot</span> is a visualization technique suitable for contingency tables that resembles a standardized stacked bar plot with the benefit that we still see the relative group sizes of the primary variable as well.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-21.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p><span class="badge rounded-pill bg-secondary">Pie charts</span> can be useful for giving a high-level overview to show how a set of cases break down. However, it is also diffcult to decipher details in a pie chart. While pie charts can be useful, we prefer <span class="badge rounded-pill bg-secondary">bar plots</span> for their ease in comparing groups.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-31.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Some of the more interesting investigations can be considered by examining <span class="badge rounded-pill bg-secondary">numerical data</span> across groups. The <span class="badge rounded-pill bg-secondary">side-by-side box plot</span> is a traditional tool for comparing across groups. Another useful plotting method uses <span class="badge rounded-pill bg-secondary">hollow histograms</span> to compare numerical data across groups. These are just the outlines of histograms of each group put on the same plot.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-32.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>The <span class="badge rounded-pill bg-secondary">weighted mean</span> is the same as the mean, except that it is influenced more by some observations than others. We assign weights to observations as a sort of way of describing its relative importance. In many applications, there are natural choices for weights. For example, in the county data set, population is a natural weighting factor. We'll use w<sub>1</sub> to represent the population of the first county, w<sub>2</sub> to represent the population of the second county, and so on. The label x<sub>1</sub> will represent the average income of county 1, x<sub>2</sub> for the average income of county 2, and so on. Then the mean weighted by population can be written as<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-22.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">(this equation represents the <span class="badge rounded-pill bg-secondary">weighted mean</span> of income, where the weights are given by the population values).</p>
                    <p>The weighed mean of observations x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub> using weights w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub> is given by<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-23.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></p>
                    <p>The simple mean is a weighted mean where all the weights are 1:<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-24.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></p>
                </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingTwo">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo" aria-expanded="true" aria-controls="collapseTwo">Probability</button>
            </h2>
            <div id="collapseTwo" class="accordion-collapse collapse" aria-labelledby="headingTwo" data-bs-parent="#accordionExample">
                <div class="accordion-body">
                    <h2 class="h4">Probability</h2>
                    <p>We often frame probability in terms of a <span class="badge rounded-pill bg-secondary">random process</span> giving rise to an <span class="badge rounded-pill bg-secondary">outcome</span>. The <span class="badge rounded-pill bg-secondary">probability</span> of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times. Probability is defined as a proportion, and it always takes values between 0 and 1 (inclusively). It may also be displayed as a percentage between 0% and 100%.</p>
                    <h2 class="h4">Disjoint or mutually exclusive outcomes</h2>
                    <p>Two outcomes are called <span class="badge rounded-pill bg-secondary">disjoint</span> or <span class="badge rounded-pill bg-secondary">mutually exclusive</span> if both cannot happen at the same time. For instance, if we roll a die one time, the outcomes 1 and 2 are disjoint since they cannot both occur. On the other hand, the outcomes 1 and "rolling an odd number" are not disjoint since both occur if the outcome of the roll is a 1.</p>
                    <h3 class="h5" id="addition-rule">Addition rule of disjoint outcomes</h3>
                    <p>If A<sub>1</sub> and A<sub>2</sub> represent two disjoint outcomes, then the probability that one of them occurs is given by<img src="../img/statistics-for-data-science/probability-2.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p>If there are many disjoint outcomes A<sub>1</sub>, ..., A<sub>k</sub>, then the probability that one of these outcomes will occur is<img src="../img/statistics-for-data-science/probability-3.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p>Data scientists rarely work with individual outcomes and instead consider sets or collections of outcomes. Let A represent the event where a die roll results in 1 or 2 and B represent the event that the die roll is a 4 or a 6. We write A as the set of outcomes {1, 2} and B = {4, 6}. These sets are commonly called <span class="badge rounded-pill bg-secondary">events</span>. Because A and B have no elements in common, they are <span class="badge rounded-pill bg-secondary">disjoint events</span>. The <span class="badge rounded-pill bg-secondary">Addition Rule</span> applies to both disjoint outcomes and disjoint events. The probability that one of the disjoint events A or B occurs is the sum of the separate probabilities (P(A or B) = P(A) + P(B) = 1/3 + 1/3 = 2/3).</p>
                    <h2 class="h4">Probabilities when events are not disjoint</h2>
                    <img src="../img/statistics-for-data-science/probability-6.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The events that a teen went to college or not are disjoint. It is not possible that a teen both attended and did not attend college at the same time. However, the events that a teen went to college (or not) is not disjoint with the event that a parent went to college (or not) and both may happen at the same time. If we simply attempt to add the probabilities of the events, and the events are not mutually exclusive, we may be double counting them. Let's consider the probability that a teen attends college and/or their parent has a degree:</p>
                    <img src="../img/statistics-for-data-science/probability-8.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-9.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-52.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-53.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-54.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-5.PNG" class="mx-auto d-block" alt="Probability">
                    <h3 class="h5">General addition rule</h3>
                    <p>If A and B are any two events, disjoint or not, then the probability that at least one of them will occur is<img src="../img/statistics-for-data-science/probability-1.PNG" class="mx-auto d-block" alt="Probability">where P(A and B) is the probability that both events occur.</p>
                    <p>If the events are <span class="badge rounded-pill bg-secondary">mutually exclusive</span>, then <span class="badge rounded-pill bg-secondary">P(A∩B)=0</span> (see <a href="#addition-rule">Addition rule of disjoint outcomes</a>):</p>
                    <img src="../img/statistics-for-data-science/probability-11.PNG" class="mx-auto d-block" alt="Probability">
                    <p><span class="badge rounded-pill bg-secondary">"or" is inclusive</span>: When we write "or" in statistics, we mean "and/or" unless we explicitly state otherwise. Thus, A or B occurs means A, B, or both A and B occur.</p>
                    <h2 class="h4">Probability distributions</h2>
                    <p>A <span class="badge rounded-pill bg-secondary">probability distribution</span> is a table of all disjoint outcomes and their associated probabilities:</p>
                    <img src="../img/statistics-for-data-science/probability-14.PNG" class="mx-auto d-block" alt="Probability">
                    <p>A probability distribution is a list of the possible outcomes with corresponding probabilities that satisfies three rules:</p>
                    <ol>
                        <li>The outcomes listed must be disjoint.</li>
                        <li>Each probability must be between 0 and 1.</li>
                        <li>The probabilities must total 1.</li>
                    </ol>
                    <p>Probability distributions can also be summarized in a bar plot:</p>
                    <img src="../img/statistics-for-data-science/probability-51.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-50.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">Complement of an event</h2>
                    <p>Rolling a die produces a value in the set {1, 2, 3, 4, 5, 6}. This set of all possible outcomes is called the <span class="badge rounded-pill bg-secondary">sample space (S)</span> for rolling a die. We often use the sample space to examine the scenario where an event does not occur.</p>
                    <p>Let D = {2, 3} represent the event that the outcome of a die roll is 2 or 3. Then the <span class="badge rounded-pill bg-secondary">complement</span> of D represents all outcomes in our sample space that are not in D, which is denoted by D<sup>c</sup> = {1, 4, 5, 6}. That is, D<sup>c</sup> is the set of all possible outcomes not already included in D.</p>
                    <p>The complement of event A is denoted A<sup>c</sup>, and A<sup>c</sup> represents all outcomes not in A. A and A<sup>c</sup> are mathematically related:</p>
                    <img src="../img/statistics-for-data-science/probability-12.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-13.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">Independence</h2>
                    <p>Two processes are <span class="badge rounded-pill bg-secondary">independent</span> if knowing the outcome of one provides no useful information about the outcome of the other. For instance, flipping a coin and rolling a die are two independent processes - knowing the coin was heads does not help determine the outcome of a die roll. On the other hand, stock prices usually move up or down together, so they are not independent.</p>
                    <h3 class="h5" id="multiplication-rule">Multiplication rule for independent processes</h3>
                    <p>If A and B represent events from two different and independent processes, then the probability that both A and B occur can be calculated as the product of their separate probabilities:</p>
                    <img src="../img/statistics-for-data-science/probability-17.PNG" class="mx-auto d-block" alt="Probability">
                    <p>Similarly, if there are k events A<sub>1</sub>, ..., A<sub>k</sub> from k independent processes, then the probability they all occur is<img src="../img/statistics-for-data-science/probability-18.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p>Suppose the variables handedness and sex are independent, i.e. knowing someone's sex provides no useful information about their handedness and vice-versa. Then we can compute whether a randomly selected person is right-handed and female using the <span class="badge rounded-pill bg-secondary">Multiplication Rule</span> (P(right-handed and female) = P(right-handed) &times; P(female) = 0.91 &times; 0.50 = 0.455)</p>
                    <p>Let's simulate two independent events: rolling two dice, and then calculate the probability that both dice yield a 6:</p>
                    <img src="../img/statistics-for-data-science/probability-19.PNG" class="mx-auto d-block" alt="Probability">
                    <p>According to the multiplication rule<img src="../img/statistics-for-data-science/probability-20.PNG" class="mx-auto d-block" alt="Probability">which is close to the number obtained from the simulation.</p>
                    <p>We say that two events A and B are independent if they satisfy P(A and B) = P(A) &times; P(B).</p>
                    <img src="../img/statistics-for-data-science/probability-44.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">Defining conditional probability</h2>
                    <img src="../img/statistics-for-data-science/probability-21.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The probability that a random teenager from the dataset attended college and that at least one of the teen's parents has a college degree is 231 out of 280 cases:</p>
                    <img src="../img/statistics-for-data-science/probability-22.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The probability that a random teenager from the dataset did not attend college and that at least one of the teen’s parents has a college degree is 49 out of 280 cases:</p>
                    <img src="../img/statistics-for-data-science/probability-23.PNG" class="mx-auto d-block" alt="Probability">
                    <p>These are <span class="badge rounded-pill bg-secondary">conditional probabilities</span> because we computed the probability under a condition: a parent has a college degree; the conditional probabilities add up to 1.</p>
                    <p>The conditional probability of the outcome of interest A given condition B is computed as the following:</p>
                    <img src="../img/statistics-for-data-science/probability-24.PNG" class="mx-auto d-block" alt="Probability">
                    <p>Applying the general definition to our example:</p>
                    <img src="../img/statistics-for-data-science/probability-25.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">Marginal and joint probabilities</h2>
                    <p>If a probability is based on a single variable, it is a <span class="badge rounded-pill bg-secondary">marginal probability</span>. For example, probability based solely on the teen variable is a marginal probability.</p>
                    <img src="../img/statistics-for-data-science/probability-27.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The probability of outcomes for two or more variables or processes is called a <span class="badge rounded-pill bg-secondary">joint probability</span>. For example, the probability that a child went to college when their parents did not.</p>
                    <img src="../img/statistics-for-data-science/probability-26.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">General multiplication rule</h2>
                    <p>Here we provide the General Multiplication Rule for events that might not be independent. This <span class="badge rounded-pill bg-secondary">General Multiplication Rule</span> is simply a rearrangement of the conditional probability equation.</p>
                    <img src="../img/statistics-for-data-science/probability-28.PNG" class="mx-auto d-block" alt="Probability">
                    <p>When events A and B are independent, the probability of event A is not impacted by the occurrence of event B and vice versa, so the following applies (see <a href="#multiplication-rule">Multiplication rule for independent processes</a>):</p>
                    <img src="../img/statistics-for-data-science/probability-29.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-30.PNG" class="mx-auto d-block" alt="Probability">
                    <h3 class="h5">Sum of conditional probabilities</h3>
                    <p>Let A<sub>1</sub>, ..., A<sub>k</sub> represent all the disjoint outcomes for a variable or process A. Then if B is an event, possibly for another variable or process, we have:</p>
                    <img src="../img/statistics-for-data-science/probability-31.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The rule for complements also holds when an event and its complement are conditioned on the same information:</p>
                    <img src="../img/statistics-for-data-science/probability-32.PNG" class="mx-auto d-block" alt="Probability">
                    <p><em>Example 1:</em> In your sock drawer, you have 4 blue, 5 grey, and 3 black socks. Half asleep one morning, you grab 2 socks at random and put them on. Find the probability you end up wearing: 1 - 2 blue socks; 2 - no grey socks; 3 - at least 1 black sock; 4 - a green sock; 5 - matching socks.<img src="../img/statistics-for-data-science/probability-33.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <h2 class="h4">Tree diagrams</h2>
                    <p><em>Example 2:</em><img src="../img/statistics-for-data-science/probability-4.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-7.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <h2 class="h4">Bayes' Theorem</h2>
                    <img src="../img/statistics-for-data-science/probability-34.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-35.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-36.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-37.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-38.PNG" class="mx-auto d-block" alt="Probability">
                    <p><em>Example 3:</em> Jose visits campus every Thursday evening. However, some days the parking garage is full, often due to college events. There are academic events on 35% of evenings, sporting events on 20% of evenings, and no events on 45% of evenings. When there is an academic event, the garage fills up about 25% of the time, and it fills up 70% of evenings with sporting events. On evenings when there are no events, it only fills up about 5% of the time. If Jose comes to campus and finds the garage full, what is the probability that there is a sporting event?<br>
                        Using a tree diagram:<img src="../img/statistics-for-data-science/probability-10.PNG" class="mx-auto d-block" alt="Probability">Using Bayes' Theorem:<img src="../img/statistics-for-data-science/probability-15.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p><em>Example 4:</em><img src="../img/statistics-for-data-science/probability-41.PNG" class="mx-auto d-block" alt="Probability">Using Bayes' Theorem:<img src="../img/statistics-for-data-science/probability-43.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p><em>Example 5:</em><img src="../img/statistics-for-data-science/probability-45.PNG" class="mx-auto d-block" alt="Probability">*This is a conditional probability; we want P(Bowl 1 | Vanilla), but it is not obvious how to compute it. If I asked a different question - the probability of a vanilla cookie given bowl 1 - it would be easy (P(Vanilla | Bowl 1) = 3/4). Sadly, P(A | B) is not the same as P(B | A), but there is a way to get from one to the other: Bayes's theorem.</p>
                    <p><em>Example 6:</em><img src="../img/statistics-for-data-science/probability-46.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-47.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <h2 class="h4">The diachronic interpretation</h2>
                    <p>There is another way to think of Bayes' theorem: it gives us a way to update the probability of a hypothesis, H, in light of some body of data, D. This way of thinking about Bayes' theorem is called the <span class="badge rounded-pill bg-secondary">diachronic interpretation</span>. "Diachronic" means that something is happening over time; in this case the probability of the hypotheses changes, over time, as we see new data.</p>
                    <p>Rewriting Bayes' theorem with H and D yields:</p>
                    <img src="../img/statistics-for-data-science/probability-48.PNG" class="mx-auto d-block" alt="Probability">
                    <p>In this interpretation, each term has a name:</p>
                    <ul>
                        <li>P(H) is the probability of the hypothesis before we see the data, called the prior probability, or just <span class="badge rounded-pill bg-secondary">prior</span>.</li>
                        <li>P(H | D) is what we want to compute, the probability of the hypothesis after we see the data, called the <span class="badge rounded-pill bg-secondary">posterior</span>.</li>
                        <li>P(D | H) is the probability of the data under the hypothesis, called the <span class="badge rounded-pill bg-secondary">likelihood</span>.</li>
                        <li>P(D) is the probability of the data under any hypothesis, called the <span class="badge rounded-pill bg-secondary">normalizing constant</span>.</li>
                    </ul>
                    <p><em>Example 7:</em><img src="../img/statistics-for-data-science/probability-49.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <h2 class="h4">Expectation</h2>
                    <p>Two books are assigned for a statistics class: a textbook and its corresponding study guide. The university bookstore determined 20% of enrolled students do not buy either book, 55% buy the textbook only, and 25% buy both books, and these percentages are relatively constant from one term to another. The textbook costs $137 and the study guide $33.</p>
                    <p>We call a variable or process with a numerical outcome a <span class="badge rounded-pill bg-secondary">random variable</span>, and we usually represent this random variable with a capital letter such as X, Y , or Z. The amount of money a single student will spend on her statistics books is a random variable, and we represent it by X. The possible outcomes of X are labeled with a corresponding lower case letter x and subscripts. For example, we write x<sub>1</sub> = $0, x<sub>2</sub> = $137, and x<sub>3</sub> = $170, which occur with probabilities 0.20, 0.55, and 0.25:</p>
                    <img src="../img/statistics-for-data-science/probability-16.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The <span class="badge rounded-pill bg-secondary">expected value</span> of a random variable is computed by adding each outcome weighted by its probability (E(X) = 0 &times; P(X = 0) + 137 &times; P(X = 137) + 170 &times; P(X = 170) = 0 &times; 0.20 + 137 &times; 0.55 + 170 &times; 0.25 = 117.85). </p>
                    <h3 class="h5">Expected value of a discrete random variable</h3>
                    <p>If X takes outcomes x<sub>1</sub>, ..., x<sub>k</sub> with probabilities P(X = x<sub>1</sub>), ..., P(X = x<sub>k</sub>), the expected value of X is the sum of each outcome multiplied by its corresponding probability:</p>
                    <img src="../img/statistics-for-data-science/probability-42.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The expected value for a random variable represents the <span class="badge rounded-pill bg-secondary">average outcome</span>. For example, E(X) = 117.85 represents the average amount the bookstore expects to make from a single student, which we could also write as &mu; = 117.85.</p>
                    <h2 class="h4">Variability in random variables</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">variance</span> and <span class="badge rounded-pill bg-secondary">standard deviation</span> can be used to describe the variability of a random variable. In the case of a random variable, we again compute squared deviations. However, we take their sum weighted by their corresponding probabilities, just like we did for the expectation.</p>
                    <h3 class="h5">General variance formula</h3>
                    <p>If X takes outcomes x<sub>1</sub>, ..., x<sub>k</sub> with probabilities P(X = x<sub>1</sub>), ..., P(X = x<sub>k</sub>) and expected value &mu; = E(X), then the variance of X, denoted by Var(X) or the symbol &sigma;<sup>2</sup>, is<img src="../img/statistics-for-data-science/probability-40.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p>The standard deviation of X, labeled &sigma;, is the square root of the variance.</p>
                    <p><em>Example 4:</em><img src="../img/statistics-for-data-science/probability-39.PNG" class="mx-auto d-block" alt="Probability"></p>
                </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingThree">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree" aria-expanded="true" aria-controls="collapseThree">Distribution of Random Variables</button>
            </h2>
            <div id="collapseThree" class="accordion-collapse collapse" aria-labelledby="headingThree" data-bs-parent="#accordionExample">
                <div class="accordion-body">
                    <p><span class="badge rounded-pill bg-secondary">Deterministic experiment</span>: An experiment that, when repeated, will always have the same outcome. For example, if you determine the eye colour of an individual, repeating the experiment will always result in the same colour.</p>
                    <p><span class="badge rounded-pill bg-secondary">Probabilistic or stochastic experiment</span>: A probabilistic experiment has an unknown or uncertain outcome. For example, if you count the number of ducks on a lake at a specific moment, or the number of members of a randomly chosen family.</p>
                    <p><span class="badge rounded-pill bg-secondary">Random variable</span>: The outcome of a probabilistic experiment. It can also be thought of as a function or rule that assigns a number to each outcome of a probabilistic experiment. Although the outcome of a probabilistic experiment is unknown, it cannot take just any value. Further, the probability of each possible value can be determined. When using notation associated with a random variable, upper case letters such as X or Y denote the definition of the random variable, and lower case letters like x or y denote the value of a random variable. If X is a random variable, then X is written in words and x is given as a number. For example, if we measure 12 apples: X = "Number of apples", x = 12.</p>
                    <p><span class="badge rounded-pill bg-secondary">Discrete random variable</span>: A random variable that can take on a countable number of values. For example, if we define  X  as the number of heads observed in an experiment that flips a coin 10 times.</p>
                    <p><span class="badge rounded-pill bg-secondary">Continuous random variable</span>: A random variable whose values are uncountable, and can take any value within a range - usually obtained by measuring. Measurements of time, height, weight, and distance are all examples of continuous random variables.</p>
                    <p><span class="badge rounded-pill bg-secondary">Probability distribution</span>: A table, formula, or graph that describes the values of a random variable and the probability associated with these values.</p>
                    <h2 class="h4">Simulating random experiments with Python</h2>
                    <p><em>Example 1</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-1.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-2.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-3.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Probability distribution function (PDF) for a discrete random variable</h2>
                    <p>In simple terms, a <span class="badge rounded-pill bg-secondary">probability distribution function</span> (PDF) assigns a probability to each possible value of a discrete random variable. A discrete probability distribution function has two characteristics:</p>
                    <ol>
                        <li>Each probability is between zero and one, inclusive.</li>
                        <li>The sum of the probabilities is one.</li>
                    </ol>
                    <p><em>Example 2</em>: A child psychologist is interested in the number of times a newborn baby's crying wakes its mother after midnight. For a random sample of 50 mothers, the following information was obtained. Let X = the number of times per week a newborn baby's crying wakes its mother after midnight. For this example, x = 0, 1, 2, 3, 4, 5. P(x) = probability that X takes on a value x.<img src="../img/statistics-for-data-science/distribution-of-random-variables-4.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">X takes on the values 0, 1, 2, 3, 4, 5. This is a discrete PDF because: each P(x) is between zero and one, inclusive and the sum of the probabilities is one.</p>
                    <h2 class="h4">Mean or expected value and standard deviation</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">expected value</span> is often referred to as the <span class="badge rounded-pill bg-secondary">"long-term" average or mean</span>. This means that over the long term of doing an experiment over and over, you would <span class="badge rounded-pill bg-secondary">expect</span> this average.</p>
                    <p><span class="badge rounded-pill bg-secondary">The Law of Large Numbers</span> states that, as the number of trials in a probability experiment increases, the difference between the theoretical probability of an event and the relative frequency approaches zero (the theoretical probability and the relative frequency get closer and closer together). When evaluating the long-term results of statistical experiments, we often want to know the "average" outcome. This "long-term average" is known as the <span class="badge rounded-pill bg-secondary">mean</span> or <span class="badge rounded-pill bg-secondary">expected value</span> of the experiment and is denoted by the Greek letter &mu;. In other words, after conducting many trials of an experiment, you would expect this average value.</p>
                    <p>To find the expected value or long term average, &mu;, simply multiply each value of the random variable by its probability and add the products.</p>
                    <p><em>Example 3</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-5.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 4</em>: Consider the following card game with a well-shuffled deck of cards. If you draw a red card, you win nothing. If you get a spade, you win 5 dollars. For any club, you win 10, plus an extra $20 for the ace of clubs. Create a probability model for the amount you can win at this game. Also, find the expected winnings for a single game and the standard deviation of the winnings. What is the maximum amount you would be willing to pay to play this game?<img src="../img/statistics-for-data-science/distribution-of-random-variables-6.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 5</em>: The game of European roulette involves spinning a wheel with 37 slots: 18 red, 18 black, and 1 green. A ball is spun onto the wheel and will eventually land in a slot, where each slot has an equal chance of capturing the ball. Gamblers can place bets on red or black. If the ball lands on their colour, they double their money. If it lands on another colour, they lose their money. Suppose you play roulette and bet $3 on a single round. What is the expected value and standard deviation of your total winnings? Suppose you bet $1 in three different rounds. What is the expected value and standard deviation of your total winnings? What does this say about the riskiness of the two games?<img src="../img/statistics-for-data-science/distribution-of-random-variables-7.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-8.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Uniform distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">uniform distribution</span> is the most intuitive distribution for discrete random variables. The outcome of rolling a die is the classic example of this type of distribution - there are six possible outcomes: { 1, 2, 3, 4, 5, 6 }, and each value has the same probability: 1/6. In the classical definition of probability, when there is no information about the probability of a set of possible outcomes, each one is assigned the same probability. In general, a random variable follows a uniform distribution when the output of an event or experiment is one of a set of n possible integers, and each integer has the same probability 1/n.</p>
                    <p><em>Example 6</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-9.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Bernoulli distribution</h2>
                    <p>Suppose a health insurance company found that 70% of the people they insure stay below their deductible in any given year. Each of these people can be thought of as a <span class="badge rounded-pill bg-secondary">trial</span>. We label a person a <span class="badge rounded-pill bg-secondary">success</span> if her healthcare costs do not exceed the deductible. We label a person a <span class="badge rounded-pill bg-secondary">failure</span> if she does exceed her deductible in the year. Because 70% of the individuals will not hit their deductible, we denote the <span class="badge rounded-pill bg-secondary">probability of a success</span> as p = 0.7. The <span class="badge rounded-pill bg-secondary">probability of a failure</span> is sometimes denoted with q = 1 - p, which would be 0.3 for the insurance example.</p>
                    <p>When an individual trial only has two possible outcomes, often labeled as <span class="badge rounded-pill bg-secondary">success</span> or <span class="badge rounded-pill bg-secondary">failure</span>, it is called a <span class="badge rounded-pill bg-secondary">Bernoulli random variable</span>. Bernoulli random variables are often denoted as 1 for a success and 0 for a failure.</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-10.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>If X is a random variable that takes value 1 with probability of success p and 0 with probability 1 - p, then X is a Bernoulli random variable with mean and standard deviation<img src="../img/statistics-for-data-science/distribution-of-random-variables-11.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 7</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-12.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Geometric distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">geometric distribution</span> is used to describe how many trials it takes to observe a success.</p>
                    <p>Suppose we are working at the insurance company and need to find a case where the person did not exceed her (or his) deductible as a case study. If the probability a person will not exceed her deductible is 0.7 and we are drawing people at random, what are the chances that the first person will not have exceeded her deductible, i.e. be a success? The second person? The third? What about we pull n - 1 cases before we find the first success, i.e. the first success is the n<sup>th</sup> person? (If the first success is the fifth person, then we say n = 5.)</p>
                    <p>The probability of stopping after the first person is just the chance the first person will not hit her (or his) deductible: 0.7. The probability the second person is the first to hit her deductible: P(second person is the first to hit deductible) = P(the first won't, the second will) = (0.3)(0.7) = 0.21. Likewise, the probability it will be the third case: (0.3)(0.3)(0.7) = 0.063. If the first success is on the n<sup>th</sup> person, then there are n - 1 failures and finally 1 success, which corresponds to the probability (0.3)<sup>n - 1</sup>(0.7). This is the same as (1 - 0.7)<sup>n - 1</sup>(0.7).</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-13.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>If the probability of a success in one trial is p and the probability of a failure is 1 - p, then the probability of finding the first success in the n<sup>th</sup> trial is given by<img src="../img/statistics-for-data-science/distribution-of-random-variables-14.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>The mean (i.e. expected value), variance, and standard deviation of this wait time are given by<img src="../img/statistics-for-data-science/distribution-of-random-variables-15.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>It takes, on average, 1/p trials to get a success under the geometric distribution. This mathematical result is consistent with what we would expect intuitively. If the probability of a success is high (e.g. 0.8), then we don't usually wait very long for a success: 1/0.8 = 1.25 trials on average. If the probability of a success is low (e.g. 0.1), then we would expect to view many trials before we see a success: 1/0.1 = 10 trials.</p>
                    <p><em>Example 8</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-16.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Binomial distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">binomial distribution</span> is used to describe the number of successes in a fixed number of trials. This is different from the geometric distribution, which described the number of trials we must wait before we observe a success.</p>
                    <p>Suppose the insurance agency is considering a random sample of four individuals they insure. What is the chance exactly one of them will exceed the deductible and the other three will not? Let's call the four people Ariana (A), Brittany (B), Carlton (C), and Damian (D) for convenience. P(A = exceed, B = not, C = not, D = not) = P(A = exceed) P(B = not) P(C = not) P(D = not) = (0.3)(0.7)(0.7)(0.7) = (0.7)<sup>3</sup>(0.3)<sup>1</sup> = 0.103. But there are three other scenarios: Brittany, Carlton, or Damian could have been the one to exceed the deductible. In each of these cases, the probability is again (0.7)<sup>3</sup>(0.3)<sup>1</sup>. These four scenarios exhaust all the possible ways that exactly one of these four people could have exceeded the deductible, so the total probability is 4 &times; (0.7)<sup>3</sup>(0.3)<sup>1</sup> = 0.412. The binomial distribution describes the probability of having exactly k successes in n independent Bernoulli trials with probability of a success p (n = 4, k = 3, p = 0.7).</p>
                    <p>Suppose the probability of a single trial being a success is p. Then the probability of observing exactly k successes in n independent trials is given by<img src="../img/statistics-for-data-science/distribution-of-random-variables-17.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>The mean, variance, and standard deviation of the number of observed successes are<img src="../img/statistics-for-data-science/distribution-of-random-variables-18.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>Is it binomial?</p>
                    <ol>
                        <li>The trials are independent.</li>
                        <li>The number of trials, n, is fixed.</li>
                        <li>Each trial outcome can be classified as a success or failure.</li>
                        <li>The probability of a success, p, is the same for each trial.</li>
                    </ol>
                    <p><em>Example 9</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-19.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Poisson distribution</h2>
                    <p>There are about 8 million individuals in New York City. How many individuals might we expect to be hospitalized for acute myocardial infarction (AMI), i.e. a heart attack, each day? According to historical records, the average number is about 4.4 individuals. However, we would also like to know the approximate distribution of counts. What would a histogram of the number of AMI occurrences each day look like if we recorded the daily counts over an entire year?</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-20.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>The sample mean (4.38) is similar to the historical average of 4.4. The sample standard deviation is about 2, and the histogram indicates that about 70% of the data fall between 2.4 and 6.4. The distribution's shape is unimodal and skewed to the right.</p>
                    <p>The <span class="badge rounded-pill bg-secondary">Poisson distribution</span> is often useful for estimating the number of events in a large population over a unit of time (the time unit is a day, the population is all New York City residents, and the historical rate is 4.4).</p>
                    <p>Suppose we are watching for events and the number of observed events follows a Poisson distribution with rate &lambda;. Then<img src="../img/statistics-for-data-science/distribution-of-random-variables-21.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 10</em>: A very skilled court stenographer makes one typographical error (typo) per hour on average. What probability distribution is most appropriate for calculating the probability of a given number of typos this stenographer makes in an hour? What are the mean and the standard deviation of the number of typos this stenographer makes? Would it be considered unusual if this stenographer made 4 typos in a given hour? Calculate the probability that this stenographer makes at most 2 typos in a given hour.<img src="../img/statistics-for-data-science/distribution-of-random-variables-22.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-23.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 11</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-24.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-25.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-26.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-27.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Probability density functions (PDFs) for a continuous random variable</h2>
                    <p>In the case of a continuous random variable, the probability of any individual outcome is theoretically zero because a continuous random variable is one that can assume an uncountable or infinite number of values. As such, we cannot list the possible values because there is an infinite number of them. However, we can determine the probability of a range of values.</p>
                    <p>The graph of a continuous probability distribution is a curve, and probability is represented by the area under the curve. The curve is called the <span class="badge rounded-pill bg-secondary">probability density function</span> (PDF), and we use the symbol &fnof;(x) to represent it. The following requirements apply to a probability density function &fnof;(x) whose range is a &le; x &le; b:</p>
                    <ol>
                        <li>the domain of &fnof; must be the set of all possible states of x</li>
                        <li>&fnof;(x) &ge; 0 for all x</li>
                        <li>the total area under the curve is 1</li>
                        <li>the probability that x has a value within the range a &le; x &le; b  is the area under the curve between x = a and x = b</li>
                    </ol>
                    <p>A function which describes the area under the curve is called a <span class="badge rounded-pill bg-secondary">cumulative distribution function</span> (CDF). The cumulative distribution function is used to evaluate probability as area. CDFs have the following properties:</p>
                    <ol>
                        <li>the outcomes are measured, not counted</li>
                        <li>the entire area under the curve and above the x-axis is equal to 1</li>
                        <li>probability is found for intervals (ranges) of x values rather than for individual x values</li>
                        <li>P(a &lt; x &lt; b) is the probability that the random variable X is in the interval between the values a and b; P(a &lt; x &lt; b) is the area under the curve, above the x-axis, to the right of a and the left of b</li>
                        <li>the probability that x takes on any single individual value is zero (P(x = a) = 0); the area below the curve, above the x-axis, and between x = a and x = a has no width, and therefore no area (area = 0); since the probability is equal to the area, the probability is also zero</li>
                        <li>P(a &lt; x &lt; b) is the same as P(a &le; x &le; b) since probability is equal to area</li>
                    </ol>
                    <p><em>Example 11</em>: Consider the function &fnof;(x) = 1/20 for 0 &le; x &le; 20. x = a real number. The graph of &fnof;(x) = 1/20 is a horizontal line. However, since 0 &le; x &le; 20, &fnof;(x) is restricted to the portion between x = 0 and x = 20, inclusive. The area between &fnof;(x) = 1/20 where 0 &le; x &le; 20 and the x-axis is the area of a rectangle with <span class="badge rounded-pill bg-secondary">base</span> = 20 and <span class="badge rounded-pill bg-secondary">height</span> = 1/20: <span class="badge rounded-pill bg-secondary">AREA = 20 &times; 1/20 = 1</span><img src="../img/statistics-for-data-science/distribution-of-random-variables-28.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">Suppose we want to find the area between &fnof;(x) = 1/20 and the x-axis where 0 &lt; x &lt; 2. <span class="badge rounded-pill bg-secondary">AREA = (2 – 0) &times; 1/20 = 0.1</span> (area of a rectangle = base &times; height) The area corresponds to a probability. The probability that x is between zero and two is 0.1, which can be written mathematically as P(0 &lt; x &lt; 2) = P(x &lt; 2) = 0.1.<img src="../img/statistics-for-data-science/distribution-of-random-variables-29.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">Suppose we want to find the area between &fnof;(x) = 1/20 and the x-axis where 4 &lt; x &lt; 15. <span class="badge rounded-pill bg-secondary">AREA = (15 – 4) &times; 1/20 = 0.55</span> The area corresponds to the probability P(4 &lt; x &lt; 15) = 0.55.<img src="../img/statistics-for-data-science/distribution-of-random-variables-30.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">Suppose we want to find P(x = 15). On an x-y graph, x = 15 is a vertical line. A vertical line has no width (or zero width). Therefore, P(x = 15) = base &times; height = 0 &times; 1/20 = 0.<img src="../img/statistics-for-data-science/distribution-of-random-variables-31.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Uniform distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">uniform distribution</span> is a continuous probability distribution and is concerned with events that are equally likely to occur. When working out problems that have a uniform distribution, be careful to note if the data is inclusive or exclusive of endpoints.</p>
                    <p><em>Example 12</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-32.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 13</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-33.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Normal distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">normal distribution</span> has two parameters (two numerical descriptive measures): the mean (&mu;) and the standard deviation (&sigma;). If X is a quantity to be measured that has a normal distribution with mean (&mu;) and standard deviation (&sigma;), we designate this by writing<img src="../img/statistics-for-data-science/distribution-of-random-variables-34.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>In theory, the mean is the same as the median, because the graph is symmetric about &mu;. As the notation indicates, the normal distribution depends only on the mean and the standard deviation. Since the area under the curve must equal one, a change in the standard deviation, &sigma;, causes a change in the shape of the curve; the curve becomes fatter or skinnier depending on &sigma;. A change in &mu; causes the graph to shift to the left or right. This means there are an infinite number of normal probability distributions. One of special interest is called the <span class="badge rounded-pill bg-secondary">standard normal distribution</span>.</p>
                    <p>The standard normal distribution is a normal distribution of standardized values called z-scores. A <span class="badge rounded-pill bg-secondary">z-score</span> is measured in units of the standard deviation. For example, if the mean of a normal distribution is five and the standard deviation is two, the value 11 is three standard deviations above (or to the right of) the mean. The calculation is as follows: x = &mu; + z &times; &sigma; = 5 + 3 &times; 2 = 11. The z-score is three.</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-35.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-36.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p><span class="badge rounded-pill bg-secondary">The z-score tells you how many standard deviations the value x is above (to the right of) or below (to the left of) the mean, &mu;.</span> Values of x that are larger than the mean have positive z-scores, and values of x that are smaller than the mean have negative z-scores. If x equals the mean, then x has a z-score of zero.</p>
                    <p><em>Example 14</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-37.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>If X is a random variable and has a normal distribution with mean &mu; and standard deviation &sigma;, then the <span class="badge rounded-pill bg-secondary">Empirical Rule</span> states the following:</p>
                    <ul>
                        <li>About 68% of the x values lie between –1σ and +1σ of the mean &mu; (within one standard deviation of the mean).</li>
                        <li>About 95% of the x values lie between –2σ and +2σ of the mean &mu; (within two standard deviations of the mean).</li>
                        <li>About 99.7% of the x values lie between –3σ and +3σ of the mean &mu; (within three standard deviations of the mean). Notice that almost all the x values lie within three standard deviations of the mean.</li>
                        <li>The z-scores for +1σ and –1σ are +1 and –1, respectively.</li>
                        <li>The z-scores for +2σ and –2σ are +2 and –2, respectively.</li>
                        <li>The z-scores for +3σ and –3σ are +3 and –3 respectively.</li>
                        <li>The empirical rule is also known as the 68-95-99.7 rule.</li>
                    </ul>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-38.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>The shaded area in the following graph indicates the area to the left of x. This area is represented by the probability P(X &lt; x). Normal tables, computers, and calculators provide or calculate the probability P(X &lt; x).</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-39.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>The area to the right is then P(X &gt; x) = 1 – P(X &lt; x). Remember, P(X &lt; x) = <span class="badge rounded-pill bg-secondary">Area to the left</span> of the vertical line through x. P(X &gt; x) = 1 – P(X &lt; x) = <span class="badge rounded-pill bg-secondary">Area to the right</span> of the vertical line through x. P(X &lt; x) is the same as P(X &le; x) and P(X &gt; x) is the same as P(X &ge; x) for continuous distributions.</p>
                    <p><em>Example 15</em>: Normal distribution using Python</p>
                    <div class="row">
                        <div class="col-md"><img src="../img/statistics-for-data-science/distribution-of-random-variables-40.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/distribution-of-random-variables-41.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></div>
                    </div>
                    <p><em>Example 16</em>: Probability calculation using the normal distribution<img src="../img/statistics-for-data-science/distribution-of-random-variables-42.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-43.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-44.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">The central limit theorem</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">central limit theorem</span> (clt for short) is one of the most powerful and useful ideas in all of statistics. There are two alternative forms of the theorem, and both alternatives are concerned with drawing finite samples size n from a population with a known mean, &mu;, and a known standard deviation, &sigma;. The first alternative says that if we collect samples of size n with a "large enough n," calculate each sample's mean, and create a histogram of those means, then the resulting histogram will tend to have an approximate normal bell shape. The second alternative says that if we again collect samples of size n that are "large enough," calculate the sum of each sample and create a histogram, then the resulting histogram will again tend to have a normal bell-shape.</p>
                    <p>The size of the sample, n, that is required in order to be "large enough" depends on the original population from which the samples are drawn (the sample size should be at least 30 or the data should come from a normal distribution). If the original population is far from normal, then more observations are needed for the sample means or sums to be normal. Sampling is done with replacement.</p>
                    <h3 class="h5">The central limit theorem for sample means (averages)</h3>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-45.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-46.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <h3 class="h5">The central limit theorem for sums</h3>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-47.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p><em>Example 17</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-48.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-49.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-50.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-51.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-52.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-53.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingFour">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFour" aria-expanded="true" aria-controls="collapseFour">Introduction to Statistical Inference</button>
            </h2>
            <div id="collapseFour" class="accordion-collapse collapse" aria-labelledby="headingFour" data-bs-parent="#accordionExample">
                <div class="accordion-body">
                    <p><span class="badge rounded-pill bg-secondary">Statistical inference</span> is the process by which the practitioner makes a prediction or estimation of a population parameter based on a sample statistic. It is primarily concerned with understanding and quantifying the uncertainty of parameter estimates. The process involves the statistician collecting data from a sample and evaluating the data in order to make a decision as to whether or not the data supports the claim that is made about the population. Due to sampling errors, a sample is not expected to perfectly represent the population - no matter how good our sampling method is. There are two measures to evaluate a prediction:</p>
                    <ul>
                        <li>A <span class="badge rounded-pill bg-secondary">confidence interval</span> is a range of plausible values within which we may find the true population value.</li>
                        <li>The <span class="badge rounded-pill bg-secondary">significance level</span> measures how many times the conclusion is likely to be incorrect.</li>
                    </ul>
                    <p>These two values are set a <span class="badge rounded-pill bg-secondary">priori</span> by the practitioner and they sum up to 100%. For example, if a professor predicts the score of an assignment with 95% confidence (and thus 5% significance) to be between 7.5 and 8.7, in the hypothetical case that they were able to repeat the experiment a very large number of times, they would be correct 95% of the time and wrong 5% of the time in their prediction. In other words, the final score will be within the confidence interval (7.5, 8.7) 95% of the time.</p>
                    <p>Let's suppose we want to estimate the average, or mean, of a population. The most intuitive way is to draw a representative sample and take the average. We can reasonably assume that the sample average is a good estimator of the population average. A <span class="badge rounded-pill bg-secondary">point estimate</span> is when we choose a single value of the sample statistic to estimate the population parameter. If the practitioner repeats the process, the sample statistics obtained are going to be different every time. This variability in the sample mean is described as <span class="badge rounded-pill bg-secondary">sampling variation</span>. Sampling variation causes the point estimates to vary and so each has an error associated with it. As the sample size increases, the rolling average approaches the true population average as more data becomes available.</p>
                    <p><em>Example 1:</em> This simulation allows you to observe how the rolling average converges to the true population mean as the sample size grows. For large samples, the average still fluctuates around the true mean but with very small amplitude.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistical-inference-1.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference">
                    <p>Mathematically, we say that the sample average (our <span class="badge rounded-pill bg-secondary">sample statistic</span>) converges to the true average of the population (our <span class="badge rounded-pill bg-secondary">population parameter</span>), as the sample size n approaches (tends to) infinity.</p>
                    <p>Theoretically, we will get the true population mean if we collect data on the entire population. However, in practice we almost always deal with a sample, not the entire population. Since sampling provides us with an estimate of the true value, we should be able to compute a possible <span class="badge rounded-pill bg-secondary">error</span> in the estimate. There are two main sources of error: <span class="badge rounded-pill bg-secondary">sampling error</span> and <span class="badge rounded-pill bg-secondary">bias</span>. Sampling error describes how much an estimate will tend to vary from sample to sample. It is also called the <span class="badge rounded-pill bg-secondary">sampling variation</span>. Imagine that we draw samples of the same size from any population and calculate a point estimate (mean value) for each sample. After collecting many samples we can build a distribution for these estimates so that we can build up a <span class="badge rounded-pill bg-secondary">sampling distribution</span> by recording the different estimate values of our different samples. However, the <span class="badge rounded-pill bg-secondary">Central Limit Theorem</span> (CLT) tells us that they are going to follow a normal distribution as long as the samples are reasonably independent and of similar variance. This property - together with the characteristics of the normal distribution - form the foundation of classical or traditional statistical inference. The variability of the sample means is described using standard deviation and it is called the <span class="badge rounded-pill bg-secondary">standard error of the mean</span>.</p>
                    <p><em>Example 2:</em><img src="../img/statistics-for-data-science/introduction-to-statistical-inference-2.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"><img src="../img/statistics-for-data-science/introduction-to-statistical-inference-3.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"><img src="../img/statistics-for-data-science/introduction-to-statistical-inference-4.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"><img src="../img/statistics-for-data-science/introduction-to-statistical-inference-5.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"></p>
                    <p>Thus, <span class="badge rounded-pill bg-secondary">point estimates</span> from a sample may be used to estimate population parameters. Point estimates are <span class="badge rounded-pill bg-secondary">not exact</span> and follow a sampling distribution. The standard deviation associated with an estimate is called the <span class="badge rounded-pill bg-secondary">standard error</span>. It represents the typical error or uncertainty associated with the estimate. In the case of estimating the mean, it is called the <span class="badge rounded-pill bg-secondary">standard error of the mean</span>.</p>
                    <p>Given n independent observations from a population with standard deviation &sigma;, the standard error of the sample mean is equal to:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistical-inference-6.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference">
                    <p>A reliable method to ensure sample observations are independent is to collect a simple random sample consisting of less than 10% of the population.</p>
                    <p>There is one subtle issue in the equation above: the population standard deviation &sigma; is typically unknown. This is solved by using the point estimate of the standard deviation from the sample, s. This estimate tends to be good when the sample size is at least 30 and the population distribution is not strongly skewed. When the sample size is smaller than 30, we will need to use a method to account for extra uncertainty in the standard error. If the skew condition is not met, a larger sample is needed to compensate for the extra skew.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistical-inference-7.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference">
                    <p>It is important to provide a <span class="badge rounded-pill bg-secondary">confidence interval</span> for point estimates. The objective is to give a <span class="badge rounded-pill bg-secondary">range of possible values</span> for the parameter. For example, if the practitioner wants to give a confidence level of 95% to the prediction, they will provide a 95% <span class="badge rounded-pill bg-secondary">confidence interval</span>, or in other words, a <span class="badge rounded-pill bg-secondary">range of possible values</span> that will contain the true population parameter 95% of the time (in the theoretical case that we can repeat the exercise an infinite number of times). Thus, a <span class="badge rounded-pill bg-secondary">confidence interval</span> is like fishing with a net, and it represents a range of plausible values where we are likely to find the population parameter. If we want to be more certain we will capture the fish, we might use a wider net. Likewise, we use a wider confidence interval if we want to be more certain that we capture the parameter.</p>
                    <p>When the distribution of a point estimate qualifies for the Central Limit Theorem and therefore closely follows a normal distribution, we can construct a 95% confidence interval as <span class="badge rounded-pill bg-secondary">point estimate &plusmn; 1.96 &times; SE</span> (In a normal distribution, 95% of the data is within 1.96 standard deviations of the mean.)</p>
                    <p>Suppose we want to consider confidence intervals where the confidence level is higher than 95%, such as a confidence level of 99%. To create a 99% confidence level, we must also widen our 95% interval. On the other hand, if we want an interval with lower confidence, such as 90%, we could use a slightly narrower interval than our original 95% interval. To create a 99% confidence interval, change 1.96 in the 95% confidence interval formula to be 2.58. That is, the formula for a 99% confidence interval is <span class="badge rounded-pill bg-secondary">point estimate &plusmn; 2.58 &times; SE</span></p>
                    <p>If a point estimate closely follows a normal model with standard error SE, then a confidence interval for the population parameter is <span class="badge rounded-pill bg-secondary">point estimate &plusmn; z<sup>*</sup> &times; SE</span> where z<sup>*</sup> corresponds to the confidence level selected. Figure 5.7 provides a picture of how to identify z<sup>*</sup> based on a confidence level. We select z<sup>*</sup> so that the area between -z<sup>*</sup> and z<sup>*</sup> in the standard normal distribution, N(0, 1), corresponds to the confidence level.<img src="../img/statistics-for-data-science/introduction-to-statistical-inference-15.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference">In a confidence interval, <span class="badge rounded-pill bg-secondary">z<sup>*</sup> &times; SE</span> is called the <span class="badge rounded-pill bg-secondary">margin of error</span>.</p>
                    <p>We are 90% confident that 87.1% to 90.4% of American adults support the expansion of solar power in 2018. We are 95% confident that the proportion of New York adults in October 2014 who supported a quarantine for anyone who had come into contact with an Ebola patient was between 0.796 and 0.844. We are 99% confident the proportion of Americans adults that support expanding the use of wind turbines is between 81.9% and 87.7% in 2018. First, notice that the statements are always about the population parameter, which considers all American adults for the energy polls or all New York adults for the quarantine poll. We also avoided another common mistake: incorrect language might try to describe the confidence interval as capturing the population parameter with a certain probability. Making a probability interpretation is a common error: while it might be useful to think of it as a probability, the confidence level only quantifies how plausible it is that the parameter is in the given interval. Another important consideration of confidence intervals is that they are only about the population parameter. A confidence interval says nothing about individual observations or point estimates. Confidence intervals only provide a plausible range for population parameters.</p>
                    <p><em>Example 3:</em> Simulate a uniformly distributed population of size 10,000,000. Take 10,000 random samples of size 30 and create a histogram to show the distribution of the sample means. Verify that even though the original population is uniformly distributed, the sample means follow a normal distribution.<img src="../img/statistics-for-data-science/introduction-to-statistical-inference-8.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"><img src="../img/statistics-for-data-science/introduction-to-statistical-inference-9.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"></p>
                    <p><em>Example 4:</em> By increasing the sample size we narrow our confidence interval - that is to say our prediction is more accurate.<img src="../img/statistics-for-data-science/introduction-to-statistical-inference-10.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"></p>
                    <p>There is no perfect way to check the normality condition, so instead we use two rules of thumb: If the sample size n is less than 30 and there are no clear outliers in the data, then we typically assume the data come from a nearly normal distribution to satisfy the condition. If the sample size n is at least 30 and there are no particularly extreme outliers, then we typically assume the sampling distribution of x&#772; is nearly normal, even if the underlying distribution of individual observations is not.</p>
                    <p><em>Example 5:</em> Consider the following two plots that come from simple random samples from difeerent populations. Their sample sizes are n<sub>1</sub> = 15 and n<sub>2</sub> = 50.<img src="../img/statistics-for-data-science/introduction-to-statistical-inference-16.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference">Are the independence and normality conditions met in each case? Each samples is from a simple random sample of its respective population, so the independence condition is satisfied. Let's next check the normality condition for each using the rule of thumb. The first sample has fewer than 30 observations, so we are watching for any clear outliers. None are present; while there is a small gap in the histogram on the right, this gap is small and 20% of the observations in this small sample are represented in that far right bar of the histogram, so we can hardly call these clear outliers. With no clear outliers, the normality condition is reasonably met. The second sample has a sample size greater than 30 and includes an outlier that appears to be roughly 5 times further from the center of the distribution than the next furthest observation. This is an example of a particularly extreme outlier, so the normality condition would not be satisfied.</p>
                    <p>So far, we have assumed that the population is normal and that we know the standard deviation of the population or that our sample is big enough that we can use the sample standard deviation as an unbiased estimator of the population standard deviation. But, what if we don't know the standard deviation of the population or can't estimate it? We'll find it useful to use a new distribution for inference calculations called the <span class="badge rounded-pill bg-secondary">t-distribution</span>. A t-distribution has a bell shape. However, its tails are thicker than the normal distribution's, meaning observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution. The extra thick tails of the t-distribution are exactly the correction needed to resolve the problem of using s in place of &sigma; in the SE calculation.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistical-inference-17.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference">
                    <p>The t-distribution is always centered at zero and has a single parameter: <span class="badge rounded-pill bg-secondary">degrees of freedom</span>. The degrees of freedom (df) describes the precise form of the bell-shaped t-distribution. In general, we'll use a t-distribution with df = n - 1 to model the sample mean when the sample size is n. That is, when we have more observations, the degrees of freedom will be larger and the t-distribution will look more like the standard normal distribution; when the degrees of freedom is about 30 or more, the t-distribution is nearly indistinguishable from the normal distribution.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistical-inference-18.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference">
                    <p><em>Example 6:</em> When we don't know the standard deviation of the population, the normal distribution can only be applied if the sample size is big enough.<img src="../img/statistics-for-data-science/introduction-to-statistical-inference-11.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"><img src="../img/statistics-for-data-science/introduction-to-statistical-inference-12.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"><img src="../img/statistics-for-data-science/introduction-to-statistical-inference-13.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference">The distribution of the sample means is normal only when the sample size is big enough. This is important when we don't know the standard deviation of the population and we rely on the standard deviation of the sample. Notice that the distributions have thicker tails than normal distributions. As a result, our confidence intervals will need to be wider for a given confidence level. This is the price of not knowing the standard deviation of the population.</p>
                    <p><em>Example 7:</em> When we cannot apply the normal distribution, we have to use the Student's t-distribution. We can show via a simulation how it converges to a normal distribution.<img src="../img/statistics-for-data-science/introduction-to-statistical-inference-14.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"></p>
                    <p>Thus, if we know the standard deviation of the population we can use the normal distribution. If we don't know the standard deviation of the population, we have to estimate it with the standard deviation of the sample and we should use the Student's t-distribution. When the sample size is big enough, the Student's t-distribution can be approximated by the normal distribution. How big is "big enough" depends on the symmetry of the population. The shape of the t-distribution depends on the sample size (which is related to the degrees of freedom). As a rule of thumb, if the sample size is bigger than ~25-30, we can approximate the t-distribution using the normal distribution even if we don't know the standard deviation of the population.</p>
                </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingFive">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFive" aria-expanded="true" aria-controls="collapseFive">Hypothesis Testing</button>
            </h2>
            <div id="collapseFive" class="accordion-collapse collapse" aria-labelledby="headingFive" data-bs-parent="#accordionExample">
                <div class="accordion-body">
                    <p>In statistical jargon, there are two hypotheses: the common belief that the defendant is innocent, called the <span class="badge rounded-pill bg-secondary">null hypothesis</span> (H<sub>0</sub>), and the <span class="badge rounded-pill bg-secondary">alternative hypothesis</span> that the defendant is guilty (H<sub>a</sub> or H<sub>1</sub>).</p>
                    <p>If the jury finds the defendant guilty, it is rejecting the null hypothesis in favour of the alternative: stating that there is enough evidence to conclude that the defendant is guilty beyond any reasonable doubt. If the evidence is not beyond any reasonable doubt, the jury will conclude that the defendant is not guilty. Notice that the jury does not say innocent - innocence is not something to prove, rather it is the shared premise H<sub>0</sub>. Similarly, a statistician will never accept the null hypothesis as proven, instead they will fail to reject the null hypothesis. For this reason, the alternative hypothesis is also called the <span class="badge rounded-pill bg-secondary">research hypothesis</span>.</p>
                    <p>In the case of statistical hypothesis testing, the practitioner is interested in finding out if a parameter equals a certain number. For example, consider the following hypotheses: H<sub>0</sub>: &mu; = 50; H<sub>a</sub>: &mu; &ne; 50. What does "beyond reasonable doubt" mean in this context? It is the probability of making a mistake that the practitioner is willing to accept or is comfortable with. Typically, this is set at 5%, which is called the <span class="badge rounded-pill bg-secondary">significance level</span> with an &alpha; of 0.05. To do this we need the 95% <span class="badge rounded-pill bg-secondary">confidence interval</span> - i.e. what range we would expect some measurement such as the mean to fall in at least 95% of the time if we repeated the experiment many times and the null hypothesis is true. This allows for the fact that we are making the decision based on a sample, not on the whole population, and so there will be some <span class="badge rounded-pill bg-secondary">sampling error</span> that will occasionally give us a sample that isn't representative and hence a value for our measurement which isn't accurate, making the null hypothesis appear to be incorrect. The practitioner would: 1 - calculate a 95% confidence interval assuming that H<sub>0</sub>: &mu; = 50 is true; 2 - collect a representative sample from the population; 3 - check if the point estimate falls within the confidence interval calculated. If the sample mean falls within the confidence interval under the null hypothesis, they will determine that there is no reason to think otherwise, and will fail to reject the null hypothesis. If, however, the sample mean is not in the confidence interval, the practitioner would deem the result highly improbable under the null hypothesis (less than 5%, since we set the significance level to 5%). Based on this evidence, they will reject the null in favour of the alternative, as the information provided by the sample does not support the null hypothesis.</p>
                    <p>Since we are testing for an inequality (H<sub>0</sub>: &mu; = 50; H<sub>a</sub>: &mu; &ne; 50), both high and low values are considered causes for rejection. The test in this case is called a <span class="badge rounded-pill bg-secondary">two-sided</span> or <span class="badge rounded-pill bg-secondary">two-tailed</span> test. In the graph below, the rejection areas are highlighted in red:<img src="../img/statistics-for-data-science/hypothesis-testing-1.PNG" class="mx-auto d-block" alt="Hypothesis Testing"></p>
                    <p>If our hypotheses were H<sub>0</sub>: &mu; &le; 50; H<sub>a</sub>: &mu; &gt; 50 or H<sub>0</sub>: &mu; &ge; 50; H<sub>a</sub>: &mu; &lt; 50 we would be rejecting only values that were too big or too small, respectively. In these cases, the tests are <span class="badge rounded-pill bg-secondary">one-tailed</span>. The first case is referred to as a <span class="badge rounded-pill bg-secondary">right-tailed</span> hypothesis test and the second one is a <span class="badge rounded-pill bg-secondary">left-tailed</span> test. Let's plot the right-tailed test only:<img src="../img/statistics-for-data-science/hypothesis-testing-2.PNG" class="mx-auto d-block" alt="Hypothesis Testing">Next, let's plot the left-tailed test:<img src="../img/statistics-for-data-science/hypothesis-testing-3.PNG" class="mx-auto d-block" alt="Hypothesis Testing"></p>
                    <p>We choose a two-tailed test when we are interested in change and have to consider change in both directions - positive and negative. For cases where the direction of change is known, we may use a one-tailed test. The choice of test is often dictated by the business question. If a company invested in an advertising campaign and they wanted to know if the advertising changed the sales volume, they would collect samples from points of sale and choose a two-tailed test to prove or disprove that the sales volume changed. On the other hand, if the company is only interested in whether a sales increase occurred, a one-tailed test can be used.</p>
                    <p>The are two possible types of errors. In the court example, the jury could acquit a guilty defendant or convict an innocent defendant. Similarly, the practitioner could reject a null hypothesis when the null hypothesis is true or fail to reject the null when the alternative hypothesis is true. These are called <span class="badge rounded-pill bg-secondary">Type 1</span> and <span class="badge rounded-pill bg-secondary">Type 2</span> errors, respectively. The probability of making a Type 1 error - the probability of rejecting the null when it is true - is called the &alpha; error; the probability of making a Type 2 error - the probability of failing to reject the null when the alternative hypothesis is true - is called the &beta; error: P(Type 1 error) = &alpha;; P(Type 2 error) = &beta;. In the same way that the &alpha; error is calculated by assuming H<sub>0</sub> to be true, the &beta; error is calculated by assuming H<sub>a</sub> to be true. There is a trade-off between the &alpha; and &beta; errors. For example, if we want to be very certain of not making a Type 1 error, the probability of a Type 2 error increases, and vice versa. The errors will depend on the sample size and how far apart H<sub>0</sub> and H<sub>a</sub> are. The illustration below shows how  α  and  β  are interrelated:<img src="../img/statistics-for-data-science/hypothesis-testing-4.PNG" class="mx-auto d-block" alt="Hypothesis Testing"><img src="../img/statistics-for-data-science/hypothesis-testing-5.PNG" class="mx-auto d-block" alt="Hypothesis Testing"></p>
                    <p>Hypothesis testing methods demonstrated so far are centred around the null hypothesis. This is because we are testing a certain claim we expect to be a fact and collect samples to test and confirm it. We do not know in advance what information a sample will bring us and usually the alternative is not known, while the null value is known. However, often times in experiment planning we may know what effect we would like to see that is outside what we normally consider to be true. The "normal" value is taken as the null hypothesis and the unusual effect is taken as an alternative value. It may be helpful to plan the experiment such that we will not miss important but unusual effects. In particular, we may determine an appropriate sample size where we can be 90 or 95% confident that we would detect any occurrences of important effects.</p>
                    <p><em>Example 1:</em> Suppose a hospital reorganized their ER unit and expects the mean waiting time to be reduced by 20 min. It is known that the average waiting time before was 3 hours and the standard deviation is about &sigma; = 60 min. To determine if the new system helps to reduce waiting times, we will obtain a sample by collecting data about waiting times in the ER, and we would like to know what sample size we need to be 90% sure that the waiting time has been reduced by 20 min. Before that, let's conduct hypothesis testing. Let's say we collected data about waiting times for 50 ER patients and the sample average is 2.5 hours. Can we confirm with 90% confidence that the new system is effective?<img src="../img/statistics-for-data-science/hypothesis-testing-6.PNG" class="mx-auto d-block" alt="Hypothesis Testing"><img src="../img/statistics-for-data-science/hypothesis-testing-7.PNG" class="mx-auto d-block" alt="Hypothesis Testing">Here, the null hypothesis is that the number of hours did not change. The alternative hypothesis is that the number of hours has been reduced. Thus, this is a one-tailed test, and for &alpha; = 0.10 the null hypothesis H<sub>0</sub> is rejected if the sample mean hours are in the lower 10% tail. Following the standard protocol, we shall calculate the p-value for the observed sample mean. This p-value or probability to observe the given mean value or less can be found using the cumulative distribution function.<img src="../img/statistics-for-data-science/hypothesis-testing-8.PNG" class="mx-auto d-block" alt="Hypothesis Testing">For this sample, we find a p-value = 0.0002035 much smaller than the significance level &alpha; = 0.1. We may reject the null hypothesis and our conclusion is that the collected sample confirms the reduction of waiting times in the ER unit. The null hypothesis is rejected when our sample mean falls in the red area. This red area marks the chosen significance level &alpha; = 0.1. Using the ppf() method we can find the x value which marks the beginning of that 0.1 area.<img src="../img/statistics-for-data-science/hypothesis-testing-9.PNG" class="mx-auto d-block" alt="Hypothesis Testing">Quite often in studies, we want to know how likely it is to detect an effect of interest. In our example we would like to know the probability of detecting the reduction of waiting times, which is the effect of interest in this case. This probability is called the <span class="badge rounded-pill bg-secondary">power of test</span>. If the hospital is interested in a reduction of waiting times by 20 minutes, the sample size is 50 and we want to know how likely we are to detect this effect in the study.<img src="../img/statistics-for-data-science/hypothesis-testing-10.PNG" class="mx-auto d-block" alt="Hypothesis Testing">Please note, that the new sampling distribution has the same standard error since we based our conclusions on the same sample of size of n = 50. The null will be rejected when our sample mean is to the left of the dotted line. On the "null" curve this dotted line marks the rejection region which corresponds to the defined significance level &alpha; = 0.1 (area in red). The area under the orange curve to the left of the dotted line defines the probability to detect a sample with a mean of 2.818 or less, which means the probability to detect an effect we are looking for.<img src="../img/statistics-for-data-science/hypothesis-testing-11.PNG" class="mx-auto d-block" alt="Hypothesis Testing">A reduction of 20 min. is the minimum reduction we can detect with a probability of ~86%. Most commonly we are looking for a power of test of 80% and sometimes 90%. These values are most commonly targeted, but in some cases we may be asked for 95% or even higher. Let's find what sample size is required to get a power of test of 90% and a significance level of 1%. Here, we need to find the standard error such that the critical value would cut 1% to the left under the null curve and 10% to the right under the alternative curve.<img src="../img/statistics-for-data-science/hypothesis-testing-12.PNG" class="mx-auto d-block" alt="Hypothesis Testing">This is an approximate solution as the sample size is an integer. This power of test value tells us that the probability to miss the reduction of waiting time by 20 min from the potential sample of 117 is 1 - 0.89959 = 0.100041.<img src="../img/statistics-for-data-science/hypothesis-testing-13.PNG" class="mx-auto d-block" alt="Hypothesis Testing"><img src="../img/statistics-for-data-science/hypothesis-testing-14.PNG" class="mx-auto d-block" alt="Hypothesis Testing"></p>
                    <p>The objective of a statistical hypothesis test is to: calculate a confidence interval assuming the null hypothesis is true; use the sample statistic to reject or fail to reject the null hypothesis. The null hypothesis is usually set as the common belief prior to the test, while the alternative hypothesis is what we want to prove by rejecting the null hypothesis. The practical reason for this approach is that it is a stronger result to reject the null than fail to reject it.</p>
                    <p>The steps of any hypothesis test are:</p>
                    <ol>
                        <li>Formulate the null hypothesis and the alternative hypothesis.</li>
                        <li>Specify the level of significance to be used.</li>
                        <li>Select the test statistic.</li>
                        <li>Establish the critical value or values of the test statistic.</li>
                        <li>Determine the actual value of the test statistic.</li>
                        <li>Make a decision.</li>
                    </ol>
                    <p>There are three equivalent approaches to hypothesis testing:</p>
                    <ol>
                        <li><span class="badge rounded-pill bg-secondary">Critical value</span>: The sample statistic is more extreme than the critical value calculated from the significance level &alpha;</li>
                        <li><span class="badge rounded-pill bg-secondary">Critical Z<sub>c</sub> value</span>: The Z<sub>sample</sub> value is more extreme than the critical value Z<sub>c</sub></li>
                        <li><span class="badge rounded-pill bg-secondary">p-value</span>: The probability p of the sample statistic is less than the significance level &alpha;</li>
                    </ol>
                    <p><em>Example 2:</em><img src="../img/statistics-for-data-science/hypothesis-testing-15.PNG" class="mx-auto d-block" alt="Hypothesis Testing"></p>
                    <p>So far, all concepts have been illustrated with the simplest scenario, which is when we want to know if the average of a population equals a specific value. This is a <span class="badge rounded-pill bg-secondary">one-sample hypothesis test</span> as we are with working with a sample of only one population. In other circumstances, we may want to determine if two populations have the same average. In this case, we are comparing two populations and the test is called a <span class="badge rounded-pill bg-secondary">two-sample hypothesis test</span>.</p>
                    <p>Imagine that the manufacturer of a gas additive claims that their product can reduce gas consumption by 10%. We design an experiment where different makes and models are run with and without this additive. Then, we take the gas consumption average across cars with and without the additive and perform a hypothesis test under the null hypothesis that the additive has no effect on gas consumption. Our experiment will lack sensitivity, because not only will gas consumption depend on the presence or absence of the additive, it will also depend on the intrinsic variability between makes and models. To address this issue, we can conduct a <span class="badge rounded-pill bg-secondary">paired test</span>, where the same car's gas consumption is measured with and without the additive. We can calculate the consumption difference for each car and compare them by subtracting the two and checking if they equal zero. In this way, we account for the variability among makes and models by comparing each one against itself.</p>
                    <p><em>Example 3:</em> Perform the following hypothesis tests: VIQ = 0 (one-sample t-test); female_viq = male_viq (two-sample t-test); FSIQ = PIQ (paired t-test) (Full Scale Intelligence Quotient - FSIQ, Verbal Intelligence Quotient - VIQ, Performance Intelligence Quotient - PIQ<img src="../img/statistics-for-data-science/hypothesis-testing-16.PNG" class="mx-auto d-block" alt="Hypothesis Testing"><img src="../img/statistics-for-data-science/hypothesis-testing-17.PNG" class="mx-auto d-block" alt="Hypothesis Testing"><img src="../img/statistics-for-data-science/hypothesis-testing-18.PNG" class="mx-auto d-block" alt="Hypothesis Testing"><img src="../img/statistics-for-data-science/hypothesis-testing-19.PNG" class="mx-auto d-block" alt="Hypothesis Testing"></p>
                    <p>There are two types of experiments for nominal data:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Binomial</span>:  Recall that a Bernoulli experiment has two possible outcomes: "yes/no", "purchase/don't purchase", "vote/don't vote", etc. More generally, we can code these two binary outcomes as either "0" or "1." For applications where a Bernoulli experiment is repeated n times, where n is a fixed number and the n trials are independent, we can model it with a binomial distribution. For this type of data, if we have enough observations, we can use the normal approximation as a good and convenient approximation to the binomial distribution to perform hypothesis tests on proportions.</li>
                        <li><span class="badge rounded-pill bg-secondary">Multinomial</span>: This is an extension of the binomial experiment where the possible outcomes are not restricted to just two binary choices. For this type of data, we apply the &Chi;<sup>2</sup> (Chi-Square) distribution to perform goodness-of-fit and contingency table tests.</li>
                    </ul>
                    <p><em>Example 4:</em><img src="../img/statistics-for-data-science/hypothesis-testing-20.PNG" class="mx-auto d-block" alt="Hypothesis Testing"></p>
                    <p><em>Example 5:</em><img src="../img/statistics-for-data-science/hypothesis-testing-21.PNG" class="mx-auto d-block" alt="Hypothesis Testing"></p>
                    <p><em>Example 6:</em><img src="../img/statistics-for-data-science/hypothesis-testing-22.PNG" class="mx-auto d-block" alt="Hypothesis Testing"></p>
                </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingSix">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSix" aria-expanded="true" aria-controls="collapseSix">ANOVA, Goodness of Fit, and Bootstrapping</button>
            </h2>
            <div id="collapseSix" class="accordion-collapse collapse" aria-labelledby="headingSix" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingSeven">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSeven" aria-expanded="true" aria-controls="collapseSeven">Linear Regression</button>
            </h2>
            <div id="collapseSeven" class="accordion-collapse collapse" aria-labelledby="headingSeven" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingEight">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseEight" aria-expanded="true" aria-controls="collapseEight">Logistic Regression</button>
            </h2>
            <div id="collapseEight" class="accordion-collapse collapse" aria-labelledby="headingEight" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingNine">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseNine" aria-expanded="true" aria-controls="collapseNine">Time Series</button>
            </h2>
            <div id="collapseNine" class="accordion-collapse collapse" aria-labelledby="headingNine" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingTen">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTen" aria-expanded="true" aria-controls="collapseTen">Introduction to Causal Inference Part 1</button>
            </h2>
            <div id="collapseTen" class="accordion-collapse collapse" aria-labelledby="headingTen" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingEleven">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseEleven" aria-expanded="true" aria-controls="collapseEleven">Introduction to Causal Inference Part 2</button>
            </h2>
            <div id="collapseEleven" class="accordion-collapse collapse" aria-labelledby="headingEleven" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
    </div>
</div>
<script src="../js/"></script> 
<script src="../js/bootstrap.min.js"></script>
</body>
</html>