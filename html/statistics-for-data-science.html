<!DOCTYPE html>
<html lang="en">
<head>
<title>Statistics for Data Science (University of Waterloo Professional Development)</title>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="shortcut icon" href="../img/favicon.ico">
<link rel="stylesheet" href="../css/bootstrap.min.css">
</head>
<body>
<div class="container">
    <h1 class="text-center mt-5 mb-5">Statistics for Data Science</h1>
    <p class="text-end mb-5"><em>University of Waterloo Professional Development</em></p>
    <div class="accordion" id="accordionExample">
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingOne">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">Introduction to Statistics for Data Science</button>
            </h2>
            <div id="collapseOne" class="accordion-collapse collapse" aria-labelledby="headingOne" data-bs-parent="#accordionExample">
                <div class="accordion-body">
                    <p><a href="../docs/timeline-of-statistics.pdf" download>Timeline of statistics, PDF</a></p>
                    <p><span class="badge rounded-pill bg-secondary">Statistics</span> is the study of how best to collect, analyze, and draw conclusions from data (identify the problem &#8594; collect relevant data &#8594; analyze the data &#8594; form a conclusion).</p>
                    <p>A <span class="badge rounded-pill bg-secondary">summary statistic</span> is a single number summarizing a large amount of data. For instance, the primary results of the study after 1 year could be described by two summary statistics: the proportion of people who had a stroke in the treatment (45/224 = 0.20 = 20%) and control (28/227 = 0.12 = 12%) groups.</p>
                    <p>When recording data, use a <span class="badge rounded-pill bg-secondary">data matrix</span> unless you have a very good reason to use a diferent structure. This structure allows new <span class="badge rounded-pill bg-secondary">cases/observational units</span> to be added as rows or new <span class="badge rounded-pill bg-secondary">variables</span> as new columns:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-11.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Types of data:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Quantitative</span> (or <span class="badge rounded-pill bg-secondary">numerical</span>) data deals with numbers and things you can measure objectively (dimensions such as height, width, and length; temperature and humidity; prices; area and volume).
                            <ul>
                                <li><span class="badge rounded-pill bg-secondary">Discrete</span> data is a count of something that cannot be made more precise. Typically, it involves integers (the number of children/adults/pets in your family; measured quantities; results of experiments; numerical values obtained by counting).</li>
                                <li><span class="badge rounded-pill bg-secondary">Continuous</span> data can be divided and reduced to finer and finer levels (you can measure someone's height at progressively more precise scales - meters, centimeters, millimeters, and beyond; values obtained by measuring - e.g. height of all students; all values in a given interval of numbers - e.g. federal spending).</li>
                            </ul>
                        </li>
                        <li><span class="badge rounded-pill bg-secondary">Qualitative</span> (or <span class="badge rounded-pill bg-secondary">categorical</span>) data deals with characteristics and descriptors that cannot be easily measured, but can be observed subjectively (personal tastes; textures; attractiveness; colour).
                            <ul>
                                <li><span class="badge rounded-pill bg-secondary">Ordinal</span> data: When items are assigned to categories that have some kind of implicit or natural order, such as "short, medium, or tall", the data is of ordinal nature. Another example is a survey question that asks us to rate an item on a 1 to 10 scale, with 10 being the best. This implies that 10 is better than 9, which is better than 8, and so on.</li>
                                <li><span class="badge rounded-pill bg-secondary">Nominal</span> data: Any categorical data that doesn't have an order (e.g. "blue", "red", "green").</li>
                                <li><span class="badge rounded-pill bg-secondary">Binary</span> data place things in one of two mutually exclusive categories: right/wrong, true/false, or accept/reject. It is nominal data but with only two distinct categories.</li>
                            </ul>
                        </li>
                        <li><span class="badge rounded-pill bg-secondary">Other</span> non-numerical data such as text or video data.</li>
                    </ul>
                    <p>Broadly speaking, when you measure something and give it a number value, you create <span class="badge rounded-pill bg-secondary">quantitative</span> data. When you classify or judge something, you create <span class="badge rounded-pill bg-secondary">qualitative</span> data.</p>
                    <p>Relationships between data:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Independent variables</span>: If two variables are not associated, then they are said to be independent. That is, two variables are independent if there is no evident relationship between the two.</li>
                        <li><span class="badge rounded-pill bg-secondary">Associated, or dependent</span>: When two variables show some connection with one another, they are called associated variables. Associated variables can also be called dependent variables and vice-versa.</li>
                        <li><span class="badge rounded-pill bg-secondary">Positive association</span>: Two variables are said to be positively associated when they have a linear relationship with a positive slope. That means when the value of one variables increases, the value of the other variable increases as well. For example, the amount people spend is positively associated with the money people make, if we assume that people who earn more spend more.</li>
                        <li><span class="badge rounded-pill bg-secondary">Negative association</span>: When the value of a variable goes down when the value of another variable goes up. This is often characterized by a downward trend when the variables are plotted.</li>
                    </ul>
                    <p>A <span class="badge rounded-pill bg-secondary">scatterplot</span> provides a case-by-case view of data for two numerical variables:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-1.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>When we suspect one variable might causally affect another, we label the first variable the <span class="badge rounded-pill bg-secondary">explanatory</span> variable and the second the <span class="badge rounded-pill bg-secondary">response</span> variable. Is federal spending, on average, higher or lower in areas with high rates of poverty? If we suspect poverty might affect spending, then poverty is the explanatory variable and federal spending is the response variable in the relationship.</p>
                    <p>Labeling variables as explanatory and response <span class="badge rounded-pill bg-secondary">does not guarantee</span> the relationship between them is causal, even if there is an association (also known as a <span class="badge rounded-pill bg-secondary">correlation</span>) between the two variables. It is also important to note that causation is an <span class="badge rounded-pill bg-secondary">asymmetric</span> relation ("X causes Y" is a different statement than "Y causes X"), whereas correlation is a <span class="badge rounded-pill bg-secondary">symmetric</span> relation ("X is correlated with Y" is an equivalent statement to "Y is correlated with X").</p>
                    <p>In statistics, a <span class="badge rounded-pill bg-secondary">confounding</span> variable is a variable that influences both the dependent variable and independent variable giving rise to a misleading association. Confounding is a causal concept and a confounding variable is also known as a <span class="badge rounded-pill bg-secondary">confounding factor</span>, a <span class="badge rounded-pill bg-secondary">lurking variable</span>, or a <span class="badge rounded-pill bg-secondary">confounder</span>. Suppose an observational study tracked sunscreen use and skin cancer, and it was found that the more sunscreen someone used, the more likely the person was to have skin cancer. Some previous research tells us that using sunscreen actually reduces skin cancer risk, so maybe there is another variable that can explain this hypothetical association between sunscreen usage and skin cancer. One important piece of information that is absent is sun exposure. If someone is out in the sun all day, she is more likely to use sunscreen and more likely to get skin cancer. Sun exposure is what is called a confounding variable, which is a variable that is correlated with both the explanatory and response variables.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-12.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p><span class="badge rounded-pill bg-secondary">Collinearity</span> (or <span class="badge rounded-pill bg-secondary">multicollinearity/ill-conditioning</span>) occurs when independent variables in a regression are so highly correlated that it becomes difficult or impossible to distinguish their individual effects on the dependent variable.</p>
                    <p>Data collection:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Observational studies</span> can provide evidence of a naturally occurring association between variables, but they cannot by themselves show a causal connection. For example, data may be collected via surveys, obtaining records (e.g. medical or company records), or by following a cohort of similar individuals as part of a study. In each of these situations, researchers merely observe the data as it arises. Hence, an observational study is conducted when data is collected in a way that does not directly interfere with how the data arises. Data where no treatment has been explicitly applied (or explicitly withheld) is called <span class="badge rounded-pill bg-secondary">observational data</span>.</li>
                        <li>When the possibility of a causal connection needs to be investigated, an <span class="badge rounded-pill bg-secondary">experiment</span> can be conducted. There are both explanatory and response variables in this case. To check if there really is a causal connection between the explanatory variable and the response, a sample of individuals are identified and split into groups. The individuals in each group are assigned a treatment. In statistics, a <span class="badge rounded-pill bg-secondary">treatment</span> is a generic term which refers to the specifics of how each group is handled for the purposes of an experiment.</li>
                    </ul>
                    <p>Generally, data in observational studies are collected only by monitoring what occurs, while experiments require the primary explanatory variable in a study be assigned for each subject by the researchers. Making causal conclusions based on experiments is often reasonable. However, making the same causal conclusions based on observational data can be unreliable and is not recommended. Thus, observational studies are generally only sufficient to show associations.</p>
                    <div class="row">
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-2.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-3.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-4.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-5.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                    </div>
                    <div class="row">
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-6.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-7.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></div>
                    </div>
                    <p>This is an experiment, as the researchers assigned the volunteers to a treatment group (beer or water).</p>
                    <p>Forms of observational studies:</p>
                    <ul>
                        <li>A <span class="badge rounded-pill bg-secondary">prospective study</span> identifes individuals and collects information as events unfold. For instance, medical researchers may identify and follow a group of patients over many years to assess the possible infuences of behavior on cancer risk. This prospective study recruits registered nurses and then collects data from them using questionnaires.</li>
                        <li><span class="badge rounded-pill bg-secondary">Retrospective studies</span> collect data after events have taken place, e.g. researchers may review past events in medical records.</li>
                    </ul>
                    <p>Studies where the researchers assign treatments to cases are called <span class="badge rounded-pill bg-secondary">experiments</span>. When this assignment includes randomization, e.g. using a coin flip to decide which treatment a patient receives, it is called a <span class="badge rounded-pill bg-secondary">randomized experiment</span>. Randomized experiments are fundamentally important when trying to show a causal connection between two variables. They are generally built on four principles:</p>
                    <ol>
                        <li><span class="badge rounded-pill bg-secondary">Controlling</span>. Researchers assign treatments to cases, and they do their best to control any other differences in the groups. Suppose a farmer wishes to evaluate a new fertilizer. She uses the new fertilizer on one field of crops (A), while using her current fertilizer on another field of crops (B). The irrigation system on field A has recently been repaired and provides adequate water to all of the crops, while the system on field B will not be repaired until next season. She concludes that the new fertilizer is far superior. The problem with this experiment is that the farmer has neglected to control for the effect of the differences in irrigation. This leads to <span class="badge rounded-pill bg-secondary">experimental bias</span>, the favoring of certain outcomes over others. To avoid this bias, the farmer should have tested the new fertilizer in identical conditions to the control group, which did not receive the treatment.</li>
                        <li><span class="badge rounded-pill bg-secondary">Randomization</span>. Researchers randomize patients into treatment groups to account for variables that cannot be controlled. For example, some patients may be more susceptible to a disease than others due to their dietary habits. Using randomization is the most reliable method of creating <span class="badge rounded-pill bg-secondary">homogeneous treatment groups</span>, without involving any potential biases.</li>
                        <li><span class="badge rounded-pill bg-secondary">Replication</span>. The more cases researchers observe, the more accurately they can estimate the effect of the explanatory variable on the response. In a single study, we replicate by collecting a suffciently large sample. Additionally, a group of scientists may replicate an entire study to verify an earlier finding. </li>
                        <li><span class="badge rounded-pill bg-secondary">Blocking</span>. Researchers sometimes know or suspect that variables, other than the treatment, influence the response. Under these circumstances, they may first group individuals based on this variable into blocks and then randomize cases within each block to the treatment groups. This strategy is often referred to as blocking. For instance, if we are looking at the effect of a drug on heart attacks, we might first split patients in the study into low-risk and high-risk blocks, then randomly assign half the patients from each block to the control group and the other half to the treatment group. This strategy ensures each treatment group has an equal number of low-risk and high-risk patients.</li>
                    </ol>
                    <p>A <span class="badge rounded-pill bg-secondary">population</span> is a collection of people, items, or events and includes all members of a defined group that we are studying or collecting information on for data driven decisions.</p>
                    <p>A <span class="badge rounded-pill bg-secondary">sample</span> is a small subset or fraction of a population.</p>
                    <p>A <span class="badge rounded-pill bg-secondary">parameter</span> is any summary number, like an average or percentage, that describes the entire population. For example, the population mean <span class="badge rounded-pill bg-secondary">μ</span> and the population proportion <span class="badge rounded-pill bg-secondary">p</span> are two population parameters.</p>
                    <p>A sample is a finite subset selected from the population with the objective of investigating its properties. The number of units in the sample is known as the <span class="badge rounded-pill bg-secondary">sample size</span>. A sample helps us draw conclusions about the full population. There are 2 key properties of a properly selected sample:</p>
                    <ol>
                        <li>The sample is <span class="badge rounded-pill bg-secondary">randomly</span> selected. A random sample is a group or set chosen from a larger population in a random manner that allows for each member of the larger group to have an equal chance of being chosen. What if we picked a sample by hand? It is entirely possible that the sample could be skewed to that person's interests, which may be entirely unintentional. This introduces bias into a sample.</li>
                        <li>The sample is a <span class="badge rounded-pill bg-secondary">representative</span> sample. A representative sample is a group or set chosen from a larger statistical population that adequately replicates the larger group according to whatever characteristic or quality is under study.</li>
                    </ol>
                    <p>However, although random sampling helps minimize bias, there are still ways in which bias can arise:</p>
                    <ul>
                        <li>In cases of surveys where the non-response rate is high - even if people are picked at random - caution must be taken. For instance, if only 30% of the people randomly sampled for a survey actually respond, then it is unclear whether the results are representative of the entire population. This <span class="badge rounded-pill bg-secondary">non-response bias</span> can skew results.</li>
                        <li>In <span class="badge rounded-pill bg-secondary">convenience samples</span>, only individuals easily accessible are included in the sample. For instance, if a political survey is performed by stopping people walking on Bay Street, this will not represent all of the city of Toronto.</li>
                    </ul>
                    <p>Sampling methods:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Simple random sampling</span>: Consider the salaries of Major League Baseball (MLB) players, where each player is a member of one of the league's 30 teams. To take a simple random sample of 120 baseball players and their salaries, we could write the names of that season's several hundreds of players onto slips of paper, drop the slips into a bucket, shake the bucket around until we are sure the names are all mixed up, then draw out slips until we have the sample of 120 players. In general, a sample is referred to as "simple random" if each case in the population has an equal chance of being included in the final sample and knowing that a case is included in a sample does not provide useful information about which other cases are included.<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-8.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></li>
                        <li><span class="badge rounded-pill bg-secondary">Stratified sampling</span>: The population is divided into groups called <span class="badge rounded-pill bg-secondary">strata</span>. The strata are chosen so that similar cases are grouped together, then a second sampling method, usually simple random sampling, is employed within each <span class="badge rounded-pill bg-secondary">stratum</span>. In the baseball salary example, the teams could represent the strata, since some teams have a lot more money. Then we might randomly sample 4 players from each team for a total of 120 players. Stratifed sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest. We might get a more stable estimate for the subpopulation in a stratum if the cases are very similar, leading to more precise estimates within each group. When we combine these estimates into a single estimate for the full population, that population estimate will tend to be more precise since each individual group estimate is itself more precise.<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-9.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></li>
                        <li><span class="badge rounded-pill bg-secondary">Cluster sampling</span>: In a cluster sample, we break up the population into many groups, called <span class="badge rounded-pill bg-secondary">clusters</span>. Then we sample a fixed number of clusters and include all observations from each of those clusters in the sample. It is important to note that, unlike with the strata in stratified sampling, the clusters should be microcosms, rather than subsections, of the population. Each cluster should be heterogeneous. Cluster sampling is most helpful when there is a lot of case-to-case variability within a cluster but the clusters themselves don't look very different from one another.<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-13.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></li>
                        <li><span class="badge rounded-pill bg-secondary">Multistage sampling</span>: A multistage sample is like a cluster sample, but rather than keeping all observations in each cluster, we collect a random sample within each selected cluster.<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-14.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></li>
                    </ul>
                    <p>Please note that although useful in certain circumstances, the use of stratified or cluster sampling can still be very subjective and can introduce bias into the sample.</p>
                    <p><span class="badge rounded-pill bg-secondary">Bias</span> is the intentional or unintentional favouring of one group or outcome over other potential groups or outcomes in the population:</p>
                    <ul>
                        <li><span class="badge rounded-pill bg-secondary">Selection bias</span>: The bias that results from an unrepresentative sample.
                            <ul>
                                <li><span class="badge rounded-pill bg-secondary">Undercoverage bias</span>: Occurs when some members of the population are inadequately represented in the sample.</li>
                                <li><span class="badge rounded-pill bg-secondary">Non-response bias</span>: Bias that results when respondents differ in meaningful ways from non-respondents.</li>
                                <li><span class="badge rounded-pill bg-secondary">Voluntary bias</span>: Sample members are self-selected volunteers.</li>
                            </ul>
                        </li>
                        <li><span class="badge rounded-pill bg-secondary">Response bias</span>: The bias that results from problems in the measurement process.
                            <ul>
                                <li><span class="badge rounded-pill bg-secondary">Leading questions</span>: Questions that encourage the expected answer.</li>
                                <li><span class="badge rounded-pill bg-secondary">Social desirability</span>: Responses may be biased toward what the respondents believe is socially desirable.</li>
                            </ul>
                        </li>
                    </ul>
                    <p>Most experiments try to determine whether some type of experimental treatment (or important factor) has a significant effect on an outcome. For example, does zinc help to reduce the length of a cold? Subjects who are chosen to participate in the experiment are typically divided into two groups: a treatment group and a control group. The <span class="badge rounded-pill bg-secondary">treatment group</span> consists of participants who receive the experimental treatment whose effect is being studied (in this case, zinc tablets). The <span class="badge rounded-pill bg-secondary">control group</span> consists of participants who do not receive the experimental treatment being studied. Instead, they get a placebo (a fake treatment - e.g. a sugar pill); a standard, nonexperimental treatment (such as vitamin C, in the zinc study); or no treatment at all, depending on the situation. After the experiment has been performed, the responses of those in the treatment group are compared with the responses from the control group to look for differences that are statistically significant (i.e. unlikely to have occurred just by chance). The study is ideally <span class="badge rounded-pill bg-secondary">double-blind</span> - researchers who interact with the participants and the participants themselves are all unaware of which group they belong to.</p>
                    <p>A <span class="badge rounded-pill bg-secondary">placebo</span> is a fake treatment, such as a sugar pill used in a medical trial. Placebos are given to the control group to account for a psychological phenomenon called the placebo effect, in which patients receiving a fake treatment still report having a response, as if it were the real treatment. By measuring the placebo effect in the control group, you can assess what portion of the reports from the treatment group were due to a real physical effect and what portion were likely due to the placebo effect.</p>
                    <p><span class="badge rounded-pill bg-secondary">Mean</span>: The average of a set of numbers. The mean is a common way to measure the center of a distribution of data. To find the mean number of characters in set of emails, we add up all the character counts and divide by the number of emails.</p>
                    <p><span class="badge rounded-pill bg-secondary">Median</span>: The value in middle of a sorted list. If there are an even number of observations, there will be two values in the middle of list and the median is calculated as the average of these two numbers.</p>
                    <p><span class="badge rounded-pill bg-secondary">Outliers</span> are values that are unusual compared to the rest of the dataset (i.e. especially small or large in numerical value). If we were to find the average salary of 5 employees, whose salaries are $40k, $50k, $45k, $40k and $100k, we compute mean to be sum of salaries divided by 5 which is $55k. However, this isn't the best representation of the group because most of the salaries are between $40k and $50k. The <span class="badge rounded-pill bg-secondary">mean</span> is skewed by the one large salary ($100k). In this situation we would typically use a better measure of central tendency, such as the <span class="badge rounded-pill bg-secondary">median</span>. Examining data for outliers serves many useful purposes, including identifying strong skew in the distribution, identifying possible data collection or data entry errors, and providing insight into interesting properties of the data.</p>
                    <p><span class="badge rounded-pill bg-secondary">Variance</span>: A measure of the variability of the data. Roughly the average squared distance from the mean.</p>
                    <p><span class="badge rounded-pill bg-secondary">Standard deviation</span>: Roughly describes how far away the typical observation is from the mean; the distance is called deviation. Usually about 70% of the data will be within one standard deviation of the mean and about 95% will be within two standard deviations. The standard deviation is also the square root of the variance.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-10.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Figure 2.9 shows three distributions that look quite different, but all have the same mean, variance, and standard deviation. Using modality, we can distinguish between the first plot (bimodal) and the last two (unimodal). Using skewness, we can distinguish between the last plot (right skewed) and the first two. While a picture, like a histogram, tells a more complete story, we can use modality and shape (symmetry/skew) to characterize basic information about a distribution:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-28.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>A <span class="badge rounded-pill bg-secondary">dot plot</span> is a one-variable scatterplot:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-25.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-26.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Dot plots show the exact value for each observation. This is useful for small data sets, but they can become hard to read with larger samples. Rather than showing the value of each observation, we prefer to think of the value as belonging to a <span class="badge rounded-pill bg-secondary">bin</span>. For example, in the loan50 data set, we created a table of counts for the number of loans with interest rates between 5.0% and 7.5%, then the number of loans with rates between 7.5% and 10.0%, and so on. Observations that fall on the boundary of a bin (e.g. 10.00%) are allocated to the lower bin. <span class="badge rounded-pill bg-secondary">Histograms</span> provide a view of the <span class="badge rounded-pill bg-secondary">data density</span>. Higher bars represent where the data are relatively more common. Histograms are especially convenient for describing the <span class="badge rounded-pill bg-secondary">shape</span> of the data distribution. The chosen <span class="badge rounded-pill bg-secondary">bin width</span> can alter the story the histogram is telling. When data trail off in one direction, the distribution has a <span class="badge rounded-pill bg-secondary">long tail</span>. If a distribution has a long left tail, it is <span class="badge rounded-pill bg-secondary">left skewed</span>. If a distribution has a long right tail, it is <span class="badge rounded-pill bg-secondary">right skewed</span>. In addition to looking at whether a distribution is skewed or <span class="badge rounded-pill bg-secondary">symmetric</span>, histograms can be used to identify modes. A <span class="badge rounded-pill bg-secondary">mode</span> is represented by a prominent peak in the distribution.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-27.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-15.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-16.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Unimodal and right skewed, with a potentially unusual observation at 60 hours/week.</p>
                    <p>A <span class="badge rounded-pill bg-secondary">box plot</span> summarizes a data set using five statistics while also plotting unusual observations. Figure 2.10 provides a vertical dot plot alongside a box plot of the interest rate variable from the loan50 data set:</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-17.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>The most common definition of a <span class="badge rounded-pill bg-secondary">percentile</span> is a number where a certain percentage of scores fall below that number. You might know that you scored 67 out of 90 on a test. But that figure has no real meaning unless you know what percentile you fall into. If you know that your score is in the 90th percentile, that means you scored better than 90% of people who took the test.</p>
                    <p>The 25th percentile is also called the <span class="badge rounded-pill bg-secondary">first quartile</span>, Q1. The 50th percentile is also called the <span class="badge rounded-pill bg-secondary">median</span>. The 75th percentile is also called the <span class="badge rounded-pill bg-secondary">third quartile</span>, Q3. Between Q1 and Q3 is the middle 50% of the data. The range these data span is called the <span class="badge rounded-pill bg-secondary">interquartile range</span>, or the IQR (IQR = Q3 - Q1).</p>
                    <p>The box in a <span class="badge rounded-pill bg-secondary">box plot</span> represents the middle 50% of the data, and the thick line in the box is the median. Whiskers of a box plot can extend up to 1.5 x IQR away from the quartiles: max upper whisker reach = Q3 + 1.5 x IQR, max lower whisker reach = Q1 - 1.5 x IQR (IQR: 20 - 10 = 10, max upper whisker reach = 20 + 1.5 x 10 = 35, max lower whisker reach = 10 - 1.5 x 10 = -5). A potential outlier is defined as an observation beyond the maximum reach of the whiskers. It is an observation that appears extreme relative to the rest of the data.</p>
                    <p>For skewed distributions it is often more helpful to use <span class="badge rounded-pill bg-secondary">median</span> and <span class="badge rounded-pill bg-secondary">IQR</span> to describe the center and spread; for symmetric distributions it is often more helpful to use the <span class="badge rounded-pill bg-secondary">mean</span> and <span class="badge rounded-pill bg-secondary">SD</span> to describe the center and spread. If the distribution is symmetric, center is often defined as the mean (mean ~ median), if the distribution is skewed or has extreme outliers, center is often defined as the median (<span class="badge rounded-pill bg-secondary">right-skewed</span>: mean &gt; median, <span class="badge rounded-pill bg-secondary">left-skewed</span>: mean &lt; median). The median and IQR are called <span class="badge rounded-pill bg-secondary">robust statistics</span> because extreme observations have little effect on their values: moving the most extreme value generally has little influence on these statistics. On the other hand, the mean and standard deviation are more heavily influenced by changes in extreme observations, which can be important in some situations.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-18.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>A table that summarizes data for two categorical variables is called a <span class="badge rounded-pill bg-secondary">contingency table</span>. Each value in the table represents the number of times a particular combination of variable outcomes occurred.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-19.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>A <span class="badge rounded-pill bg-secondary">bar plot</span> is a common way to display a single categorical variable. A bar plot where proportions instead of frequencies are shown is called a <span class="badge rounded-pill bg-secondary">relative frequency bar plot</span>.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-20.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p><span class="badge rounded-pill bg-secondary">Bar plots</span> are used for displaying distributions of categorical variables, while <span class="badge rounded-pill bg-secondary">histograms</span> are used for numerical variables. The x-axis in a histogram is a number line, hence the order of the bars cannot be changed, while in a bar plot the categories can be listed in any order (though some orderings make more sense than others, especially for ordinal variables).</p>
                    <p><span class="badge rounded-pill bg-secondary">Stacked bar plot</span> is a graphical display of contingency table information, for counts. <span class="badge rounded-pill bg-secondary">Side-by-side bar plot</span> displays the same information by placing bars next to, instead of on top of, each other. <span class="badge rounded-pill bg-secondary">Standardized stacked bar plot</span> is a graphical display of contingency table information, for proportions.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-29.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"><img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-30.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>A <span class="badge rounded-pill bg-secondary">mosaic plot</span> is a visualization technique suitable for contingency tables that resembles a standardized stacked bar plot with the benefit that we still see the relative group sizes of the primary variable as well.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-21.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p><span class="badge rounded-pill bg-secondary">Pie charts</span> can be useful for giving a high-level overview to show how a set of cases break down. However, it is also diffcult to decipher details in a pie chart. While pie charts can be useful, we prefer <span class="badge rounded-pill bg-secondary">bar plots</span> for their ease in comparing groups.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-31.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>Some of the more interesting investigations can be considered by examining <span class="badge rounded-pill bg-secondary">numerical data</span> across groups. The <span class="badge rounded-pill bg-secondary">side-by-side box plot</span> is a traditional tool for comparing across groups. Another useful plotting method uses <span class="badge rounded-pill bg-secondary">hollow histograms</span> to compare numerical data across groups. These are just the outlines of histograms of each group put on the same plot.</p>
                    <img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-32.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">
                    <p>The <span class="badge rounded-pill bg-secondary">weighted mean</span> is the same as the mean, except that it is influenced more by some observations than others. We assign weights to observations as a sort of way of describing its relative importance. In many applications, there are natural choices for weights. For example, in the county data set, population is a natural weighting factor. We'll use w<sub>1</sub> to represent the population of the first county, w<sub>2</sub> to represent the population of the second county, and so on. The label x<sub>1</sub> will represent the average income of county 1, x<sub>2</sub> for the average income of county 2, and so on. Then the mean weighted by population can be written as<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-22.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science">(this equation represents the <span class="badge rounded-pill bg-secondary">weighted mean</span> of income, where the weights are given by the population values).</p>
                    <p>The weighed mean of observations x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub> using weights w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub> is given by<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-23.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></p>
                    <p>The simple mean is a weighted mean where all the weights are 1:<img src="../img/statistics-for-data-science/introduction-to-statistics-for-data-science-24.PNG" class="mx-auto d-block" alt="Introduction to Statistics for Data Science"></p>
                </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingTwo">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo" aria-expanded="true" aria-controls="collapseTwo">Probability</button>
            </h2>
            <div id="collapseTwo" class="accordion-collapse collapse" aria-labelledby="headingTwo" data-bs-parent="#accordionExample">
                <div class="accordion-body">
                    <h2 class="h4">Probability</h2>
                    <p>We often frame probability in terms of a <span class="badge rounded-pill bg-secondary">random process</span> giving rise to an <span class="badge rounded-pill bg-secondary">outcome</span>. The <span class="badge rounded-pill bg-secondary">probability</span> of an outcome is the proportion of times the outcome would occur if we observed the random process an infinite number of times. Probability is defined as a proportion, and it always takes values between 0 and 1 (inclusively). It may also be displayed as a percentage between 0% and 100%.</p>
                    <h2 class="h4">Disjoint or mutually exclusive outcomes</h2>
                    <p>Two outcomes are called <span class="badge rounded-pill bg-secondary">disjoint</span> or <span class="badge rounded-pill bg-secondary">mutually exclusive</span> if both cannot happen at the same time. For instance, if we roll a die one time, the outcomes 1 and 2 are disjoint since they cannot both occur. On the other hand, the outcomes 1 and "rolling an odd number" are not disjoint since both occur if the outcome of the roll is a 1.</p>
                    <h3 class="h5" id="addition-rule">Addition rule of disjoint outcomes</h3>
                    <p>If A<sub>1</sub> and A<sub>2</sub> represent two disjoint outcomes, then the probability that one of them occurs is given by<img src="../img/statistics-for-data-science/probability-2.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p>If there are many disjoint outcomes A<sub>1</sub>, ..., A<sub>k</sub>, then the probability that one of these outcomes will occur is<img src="../img/statistics-for-data-science/probability-3.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p>Data scientists rarely work with individual outcomes and instead consider sets or collections of outcomes. Let A represent the event where a die roll results in 1 or 2 and B represent the event that the die roll is a 4 or a 6. We write A as the set of outcomes {1, 2} and B = {4, 6}. These sets are commonly called <span class="badge rounded-pill bg-secondary">events</span>. Because A and B have no elements in common, they are <span class="badge rounded-pill bg-secondary">disjoint events</span>. The <span class="badge rounded-pill bg-secondary">Addition Rule</span> applies to both disjoint outcomes and disjoint events. The probability that one of the disjoint events A or B occurs is the sum of the separate probabilities (P(A or B) = P(A) + P(B) = 1/3 + 1/3 = 2/3).</p>
                    <h2 class="h4">Probabilities when events are not disjoint</h2>
                    <img src="../img/statistics-for-data-science/probability-6.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The events that a teen went to college or not are disjoint. It is not possible that a teen both attended and did not attend college at the same time. However, the events that a teen went to college (or not) is not disjoint with the event that a parent went to college (or not) and both may happen at the same time. If we simply attempt to add the probabilities of the events, and the events are not mutually exclusive, we may be double counting them. Let's consider the probability that a teen attends college and/or their parent has a degree:</p>
                    <img src="../img/statistics-for-data-science/probability-8.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-9.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-52.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-53.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-54.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-5.PNG" class="mx-auto d-block" alt="Probability">
                    <h3 class="h5">General addition rule</h3>
                    <p>If A and B are any two events, disjoint or not, then the probability that at least one of them will occur is<img src="../img/statistics-for-data-science/probability-1.PNG" class="mx-auto d-block" alt="Probability">where P(A and B) is the probability that both events occur.</p>
                    <p>If the events are <span class="badge rounded-pill bg-secondary">mutually exclusive</span>, then <span class="badge rounded-pill bg-secondary">P(A∩B)=0</span> (see <a href="#addition-rule">Addition rule of disjoint outcomes</a>):</p>
                    <img src="../img/statistics-for-data-science/probability-11.PNG" class="mx-auto d-block" alt="Probability">
                    <p><span class="badge rounded-pill bg-secondary">"or" is inclusive</span>: When we write "or" in statistics, we mean "and/or" unless we explicitly state otherwise. Thus, A or B occurs means A, B, or both A and B occur.</p>
                    <h2 class="h4">Probability distributions</h2>
                    <p>A <span class="badge rounded-pill bg-secondary">probability distribution</span> is a table of all disjoint outcomes and their associated probabilities:</p>
                    <img src="../img/statistics-for-data-science/probability-14.PNG" class="mx-auto d-block" alt="Probability">
                    <p>A probability distribution is a list of the possible outcomes with corresponding probabilities that satisfies three rules:</p>
                    <ol>
                        <li>The outcomes listed must be disjoint.</li>
                        <li>Each probability must be between 0 and 1.</li>
                        <li>The probabilities must total 1.</li>
                    </ol>
                    <p>Probability distributions can also be summarized in a bar plot:</p>
                    <img src="../img/statistics-for-data-science/probability-51.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-50.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">Complement of an event</h2>
                    <p>Rolling a die produces a value in the set {1, 2, 3, 4, 5, 6}. This set of all possible outcomes is called the <span class="badge rounded-pill bg-secondary">sample space (S)</span> for rolling a die. We often use the sample space to examine the scenario where an event does not occur.</p>
                    <p>Let D = {2, 3} represent the event that the outcome of a die roll is 2 or 3. Then the <span class="badge rounded-pill bg-secondary">complement</span> of D represents all outcomes in our sample space that are not in D, which is denoted by D<sup>c</sup> = {1, 4, 5, 6}. That is, D<sup>c</sup> is the set of all possible outcomes not already included in D.</p>
                    <p>The complement of event A is denoted A<sup>c</sup>, and A<sup>c</sup> represents all outcomes not in A. A and A<sup>c</sup> are mathematically related:</p>
                    <img src="../img/statistics-for-data-science/probability-12.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-13.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">Independence</h2>
                    <p>Two processes are <span class="badge rounded-pill bg-secondary">independent</span> if knowing the outcome of one provides no useful information about the outcome of the other. For instance, flipping a coin and rolling a die are two independent processes - knowing the coin was heads does not help determine the outcome of a die roll. On the other hand, stock prices usually move up or down together, so they are not independent.</p>
                    <h3 class="h5" id="multiplication-rule">Multiplication rule for independent processes</h3>
                    <p>If A and B represent events from two different and independent processes, then the probability that both A and B occur can be calculated as the product of their separate probabilities:</p>
                    <img src="../img/statistics-for-data-science/probability-17.PNG" class="mx-auto d-block" alt="Probability">
                    <p>Similarly, if there are k events A<sub>1</sub>, ..., A<sub>k</sub> from k independent processes, then the probability they all occur is<img src="../img/statistics-for-data-science/probability-18.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p>Suppose the variables handedness and sex are independent, i.e. knowing someone's sex provides no useful information about their handedness and vice-versa. Then we can compute whether a randomly selected person is right-handed and female using the <span class="badge rounded-pill bg-secondary">Multiplication Rule</span> (P(right-handed and female) = P(right-handed) &times; P(female) = 0.91 &times; 0.50 = 0.455)</p>
                    <p>Let's simulate two independent events: rolling two dice, and then calculate the probability that both dice yield a 6:</p>
                    <img src="../img/statistics-for-data-science/probability-19.PNG" class="mx-auto d-block" alt="Probability">
                    <p>According to the multiplication rule<img src="../img/statistics-for-data-science/probability-20.PNG" class="mx-auto d-block" alt="Probability">which is close to the number obtained from the simulation.</p>
                    <p>We say that two events A and B are independent if they satisfy P(A and B) = P(A) &times; P(B).</p>
                    <img src="../img/statistics-for-data-science/probability-44.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">Defining conditional probability</h2>
                    <img src="../img/statistics-for-data-science/probability-21.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The probability that a random teenager from the dataset attended college and that at least one of the teen's parents has a college degree is 231 out of 280 cases:</p>
                    <img src="../img/statistics-for-data-science/probability-22.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The probability that a random teenager from the dataset did not attend college and that at least one of the teen’s parents has a college degree is 49 out of 280 cases:</p>
                    <img src="../img/statistics-for-data-science/probability-23.PNG" class="mx-auto d-block" alt="Probability">
                    <p>These are <span class="badge rounded-pill bg-secondary">conditional probabilities</span> because we computed the probability under a condition: a parent has a college degree; the conditional probabilities add up to 1.</p>
                    <p>The conditional probability of the outcome of interest A given condition B is computed as the following:</p>
                    <img src="../img/statistics-for-data-science/probability-24.PNG" class="mx-auto d-block" alt="Probability">
                    <p>Applying the general definition to our example:</p>
                    <img src="../img/statistics-for-data-science/probability-25.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">Marginal and joint probabilities</h2>
                    <p>If a probability is based on a single variable, it is a <span class="badge rounded-pill bg-secondary">marginal probability</span>. For example, probability based solely on the teen variable is a marginal probability.</p>
                    <img src="../img/statistics-for-data-science/probability-27.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The probability of outcomes for two or more variables or processes is called a <span class="badge rounded-pill bg-secondary">joint probability</span>. For example, the probability that a child went to college when their parents did not.</p>
                    <img src="../img/statistics-for-data-science/probability-26.PNG" class="mx-auto d-block" alt="Probability">
                    <h2 class="h4">General multiplication rule</h2>
                    <p>Here we provide the General Multiplication Rule for events that might not be independent. This <span class="badge rounded-pill bg-secondary">General Multiplication Rule</span> is simply a rearrangement of the conditional probability equation.</p>
                    <img src="../img/statistics-for-data-science/probability-28.PNG" class="mx-auto d-block" alt="Probability">
                    <p>When events A and B are independent, the probability of event A is not impacted by the occurrence of event B and vice versa, so the following applies (see <a href="#multiplication-rule">Multiplication rule for independent processes</a>):</p>
                    <img src="../img/statistics-for-data-science/probability-29.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-30.PNG" class="mx-auto d-block" alt="Probability">
                    <h3 class="h5">Sum of conditional probabilities</h3>
                    <p>Let A<sub>1</sub>, ..., A<sub>k</sub> represent all the disjoint outcomes for a variable or process A. Then if B is an event, possibly for another variable or process, we have:</p>
                    <img src="../img/statistics-for-data-science/probability-31.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The rule for complements also holds when an event and its complement are conditioned on the same information:</p>
                    <img src="../img/statistics-for-data-science/probability-32.PNG" class="mx-auto d-block" alt="Probability">
                    <p><em>Example 1:</em> In your sock drawer, you have 4 blue, 5 grey, and 3 black socks. Half asleep one morning, you grab 2 socks at random and put them on. Find the probability you end up wearing: 1 - 2 blue socks; 2 - no grey socks; 3 - at least 1 black sock; 4 - a green sock; 5 - matching socks.<img src="../img/statistics-for-data-science/probability-33.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <h2 class="h4">Tree diagrams</h2>
                    <p><em>Example 2:</em><img src="../img/statistics-for-data-science/probability-4.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-7.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <h2 class="h4">Bayes' Theorem</h2>
                    <img src="../img/statistics-for-data-science/probability-34.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-35.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-36.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-37.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-38.PNG" class="mx-auto d-block" alt="Probability">
                    <p><em>Example 3:</em> Jose visits campus every Thursday evening. However, some days the parking garage is full, often due to college events. There are academic events on 35% of evenings, sporting events on 20% of evenings, and no events on 45% of evenings. When there is an academic event, the garage fills up about 25% of the time, and it fills up 70% of evenings with sporting events. On evenings when there are no events, it only fills up about 5% of the time. If Jose comes to campus and finds the garage full, what is the probability that there is a sporting event?<br>
                        Using a tree diagram:<img src="../img/statistics-for-data-science/probability-10.PNG" class="mx-auto d-block" alt="Probability">Using Bayes' Theorem:<img src="../img/statistics-for-data-science/probability-15.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p><em>Example 4:</em><img src="../img/statistics-for-data-science/probability-41.PNG" class="mx-auto d-block" alt="Probability">Using Bayes' Theorem:<img src="../img/statistics-for-data-science/probability-43.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p><em>Example 5:</em><img src="../img/statistics-for-data-science/probability-45.PNG" class="mx-auto d-block" alt="Probability">*This is a conditional probability; we want P(Bowl 1 | Vanilla), but it is not obvious how to compute it. If I asked a different question - the probability of a vanilla cookie given bowl 1 - it would be easy (P(Vanilla | Bowl 1) = 3/4). Sadly, P(A | B) is not the same as P(B | A), but there is a way to get from one to the other: Bayes's theorem.</p>
                    <p><em>Example 6:</em><img src="../img/statistics-for-data-science/probability-46.PNG" class="mx-auto d-block" alt="Probability"><img src="../img/statistics-for-data-science/probability-47.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <h2 class="h4">The diachronic interpretation</h2>
                    <p>There is another way to think of Bayes' theorem: it gives us a way to update the probability of a hypothesis, H, in light of some body of data, D. This way of thinking about Bayes' theorem is called the <span class="badge rounded-pill bg-secondary">diachronic interpretation</span>. "Diachronic" means that something is happening over time; in this case the probability of the hypotheses changes, over time, as we see new data.</p>
                    <p>Rewriting Bayes' theorem with H and D yields:</p>
                    <img src="../img/statistics-for-data-science/probability-48.PNG" class="mx-auto d-block" alt="Probability">
                    <p>In this interpretation, each term has a name:</p>
                    <ul>
                        <li>P(H) is the probability of the hypothesis before we see the data, called the prior probability, or just <span class="badge rounded-pill bg-secondary">prior</span>.</li>
                        <li>P(H | D) is what we want to compute, the probability of the hypothesis after we see the data, called the <span class="badge rounded-pill bg-secondary">posterior</span>.</li>
                        <li>P(D | H) is the probability of the data under the hypothesis, called the <span class="badge rounded-pill bg-secondary">likelihood</span>.</li>
                        <li>P(D) is the probability of the data under any hypothesis, called the <span class="badge rounded-pill bg-secondary">normalizing constant</span>.</li>
                    </ul>
                    <p><em>Example 7:</em><img src="../img/statistics-for-data-science/probability-49.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <h2 class="h4">Expectation</h2>
                    <p>Two books are assigned for a statistics class: a textbook and its corresponding study guide. The university bookstore determined 20% of enrolled students do not buy either book, 55% buy the textbook only, and 25% buy both books, and these percentages are relatively constant from one term to another. The textbook costs $137 and the study guide $33.</p>
                    <p>We call a variable or process with a numerical outcome a <span class="badge rounded-pill bg-secondary">random variable</span>, and we usually represent this random variable with a capital letter such as X, Y , or Z. The amount of money a single student will spend on her statistics books is a random variable, and we represent it by X. The possible outcomes of X are labeled with a corresponding lower case letter x and subscripts. For example, we write x<sub>1</sub> = $0, x<sub>2</sub> = $137, and x<sub>3</sub> = $170, which occur with probabilities 0.20, 0.55, and 0.25:</p>
                    <img src="../img/statistics-for-data-science/probability-16.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The <span class="badge rounded-pill bg-secondary">expected value</span> of a random variable is computed by adding each outcome weighted by its probability (E(X) = 0 &times; P(X = 0) + 137 &times; P(X = 137) + 170 &times; P(X = 170) = 0 &times; 0.20 + 137 &times; 0.55 + 170 &times; 0.25 = 117.85). </p>
                    <h3 class="h5">Expected value of a discrete random variable</h3>
                    <p>If X takes outcomes x<sub>1</sub>, ..., x<sub>k</sub> with probabilities P(X = x<sub>1</sub>), ..., P(X = x<sub>k</sub>), the expected value of X is the sum of each outcome multiplied by its corresponding probability:</p>
                    <img src="../img/statistics-for-data-science/probability-42.PNG" class="mx-auto d-block" alt="Probability">
                    <p>The expected value for a random variable represents the <span class="badge rounded-pill bg-secondary">average outcome</span>. For example, E(X) = 117.85 represents the average amount the bookstore expects to make from a single student, which we could also write as &mu; = 117.85.</p>
                    <h2 class="h4">Variability in random variables</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">variance</span> and <span class="badge rounded-pill bg-secondary">standard deviation</span> can be used to describe the variability of a random variable. In the case of a random variable, we again compute squared deviations. However, we take their sum weighted by their corresponding probabilities, just like we did for the expectation.</p>
                    <h3 class="h5">General variance formula</h3>
                    <p>If X takes outcomes x<sub>1</sub>, ..., x<sub>k</sub> with probabilities P(X = x<sub>1</sub>), ..., P(X = x<sub>k</sub>) and expected value &mu; = E(X), then the variance of X, denoted by Var(X) or the symbol &sigma;<sup>2</sup>, is<img src="../img/statistics-for-data-science/probability-40.PNG" class="mx-auto d-block" alt="Probability"></p>
                    <p>The standard deviation of X, labeled &sigma;, is the square root of the variance.</p>
                    <p><em>Example 4:</em><img src="../img/statistics-for-data-science/probability-39.PNG" class="mx-auto d-block" alt="Probability"></p>
                </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingThree">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree" aria-expanded="true" aria-controls="collapseThree">Distribution of Random Variables</button>
            </h2>
            <div id="collapseThree" class="accordion-collapse collapse" aria-labelledby="headingThree" data-bs-parent="#accordionExample">
                <div class="accordion-body">
                    <p><span class="badge rounded-pill bg-secondary">Deterministic experiment</span>: An experiment that, when repeated, will always have the same outcome. For example, if you determine the eye colour of an individual, repeating the experiment will always result in the same colour.</p>
                    <p><span class="badge rounded-pill bg-secondary">Probabilistic or stochastic experiment</span>: A probabilistic experiment has an unknown or uncertain outcome. For example, if you count the number of ducks on a lake at a specific moment, or the number of members of a randomly chosen family.</p>
                    <p><span class="badge rounded-pill bg-secondary">Random variable</span>: The outcome of a probabilistic experiment. It can also be thought of as a function or rule that assigns a number to each outcome of a probabilistic experiment. Although the outcome of a probabilistic experiment is unknown, it cannot take just any value. Further, the probability of each possible value can be determined. When using notation associated with a random variable, upper case letters such as X or Y denote the definition of the random variable, and lower case letters like x or y denote the value of a random variable. If X is a random variable, then X is written in words and x is given as a number. For example, if we measure 12 apples: X = "Number of apples", x = 12.</p>
                    <p><span class="badge rounded-pill bg-secondary">Discrete random variable</span>: A random variable that can take on a countable number of values. For example, if we define  X  as the number of heads observed in an experiment that flips a coin 10 times.</p>
                    <p><span class="badge rounded-pill bg-secondary">Continuous random variable</span>: A random variable whose values are uncountable, and can take any value within a range - usually obtained by measuring. Measurements of time, height, weight, and distance are all examples of continuous random variables.</p>
                    <p><span class="badge rounded-pill bg-secondary">Probability distribution</span>: A table, formula, or graph that describes the values of a random variable and the probability associated with these values.</p>
                    <h2 class="h4">Simulating random experiments with Python</h2>
                    <p><em>Example 1</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-1.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-2.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-3.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Probability distribution function (PDF) for a discrete random variable</h2>
                    <p>In simple terms, a <span class="badge rounded-pill bg-secondary">probability distribution function</span> (PDF) assigns a probability to each possible value of a discrete random variable. A discrete probability distribution function has two characteristics:</p>
                    <ol>
                        <li>Each probability is between zero and one, inclusive.</li>
                        <li>The sum of the probabilities is one.</li>
                    </ol>
                    <p><em>Example 2</em>: A child psychologist is interested in the number of times a newborn baby's crying wakes its mother after midnight. For a random sample of 50 mothers, the following information was obtained. Let X = the number of times per week a newborn baby's crying wakes its mother after midnight. For this example, x = 0, 1, 2, 3, 4, 5. P(x) = probability that X takes on a value x.<img src="../img/statistics-for-data-science/distribution-of-random-variables-4.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">X takes on the values 0, 1, 2, 3, 4, 5. This is a discrete PDF because: each P(x) is between zero and one, inclusive and the sum of the probabilities is one.</p>
                    <h2 class="h4">Mean or expected value and standard deviation</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">expected value</span> is often referred to as the <span class="badge rounded-pill bg-secondary">"long-term" average or mean</span>. This means that over the long term of doing an experiment over and over, you would <span class="badge rounded-pill bg-secondary">expect</span> this average.</p>
                    <p><span class="badge rounded-pill bg-secondary">The Law of Large Numbers</span> states that, as the number of trials in a probability experiment increases, the difference between the theoretical probability of an event and the relative frequency approaches zero (the theoretical probability and the relative frequency get closer and closer together). When evaluating the long-term results of statistical experiments, we often want to know the "average" outcome. This "long-term average" is known as the <span class="badge rounded-pill bg-secondary">mean</span> or <span class="badge rounded-pill bg-secondary">expected value</span> of the experiment and is denoted by the Greek letter &mu;. In other words, after conducting many trials of an experiment, you would expect this average value.</p>
                    <p>To find the expected value or long term average, &mu;, simply multiply each value of the random variable by its probability and add the products.</p>
                    <p><em>Example 3</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-5.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 4</em>: Consider the following card game with a well-shuffled deck of cards. If you draw a red card, you win nothing. If you get a spade, you win 5 dollars. For any club, you win 10, plus an extra $20 for the ace of clubs. Create a probability model for the amount you can win at this game. Also, find the expected winnings for a single game and the standard deviation of the winnings. What is the maximum amount you would be willing to pay to play this game?<img src="../img/statistics-for-data-science/distribution-of-random-variables-6.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 5</em>: The game of European roulette involves spinning a wheel with 37 slots: 18 red, 18 black, and 1 green. A ball is spun onto the wheel and will eventually land in a slot, where each slot has an equal chance of capturing the ball. Gamblers can place bets on red or black. If the ball lands on their colour, they double their money. If it lands on another colour, they lose their money. Suppose you play roulette and bet $3 on a single round. What is the expected value and standard deviation of your total winnings? Suppose you bet $1 in three different rounds. What is the expected value and standard deviation of your total winnings? What does this say about the riskiness of the two games?<img src="../img/statistics-for-data-science/distribution-of-random-variables-7.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-8.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Uniform distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">uniform distribution</span> is the most intuitive distribution for discrete random variables. The outcome of rolling a die is the classic example of this type of distribution - there are six possible outcomes: { 1, 2, 3, 4, 5, 6 }, and each value has the same probability: 1/6. In the classical definition of probability, when there is no information about the probability of a set of possible outcomes, each one is assigned the same probability. In general, a random variable follows a uniform distribution when the output of an event or experiment is one of a set of n possible integers, and each integer has the same probability 1/n.</p>
                    <p><em>Example 6</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-9.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Bernoulli distribution</h2>
                    <p>Suppose a health insurance company found that 70% of the people they insure stay below their deductible in any given year. Each of these people can be thought of as a <span class="badge rounded-pill bg-secondary">trial</span>. We label a person a <span class="badge rounded-pill bg-secondary">success</span> if her healthcare costs do not exceed the deductible. We label a person a <span class="badge rounded-pill bg-secondary">failure</span> if she does exceed her deductible in the year. Because 70% of the individuals will not hit their deductible, we denote the <span class="badge rounded-pill bg-secondary">probability of a success</span> as p = 0.7. The <span class="badge rounded-pill bg-secondary">probability of a failure</span> is sometimes denoted with q = 1 - p, which would be 0.3 for the insurance example.</p>
                    <p>When an individual trial only has two possible outcomes, often labeled as <span class="badge rounded-pill bg-secondary">success</span> or <span class="badge rounded-pill bg-secondary">failure</span>, it is called a <span class="badge rounded-pill bg-secondary">Bernoulli random variable</span>. Bernoulli random variables are often denoted as 1 for a success and 0 for a failure.</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-10.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>If X is a random variable that takes value 1 with probability of success p and 0 with probability 1 - p, then X is a Bernoulli random variable with mean and standard deviation<img src="../img/statistics-for-data-science/distribution-of-random-variables-11.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 7</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-12.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Geometric distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">geometric distribution</span> is used to describe how many trials it takes to observe a success.</p>
                    <p>Suppose we are working at the insurance company and need to find a case where the person did not exceed her (or his) deductible as a case study. If the probability a person will not exceed her deductible is 0.7 and we are drawing people at random, what are the chances that the first person will not have exceeded her deductible, i.e. be a success? The second person? The third? What about we pull n - 1 cases before we find the first success, i.e. the first success is the n<sup>th</sup> person? (If the first success is the fifth person, then we say n = 5.)</p>
                    <p>The probability of stopping after the first person is just the chance the first person will not hit her (or his) deductible: 0.7. The probability the second person is the first to hit her deductible: P(second person is the first to hit deductible) = P(the first won't, the second will) = (0.3)(0.7) = 0.21. Likewise, the probability it will be the third case: (0.3)(0.3)(0.7) = 0.063. If the first success is on the n<sup>th</sup> person, then there are n - 1 failures and finally 1 success, which corresponds to the probability (0.3)<sup>n - 1</sup>(0.7). This is the same as (1 - 0.7)<sup>n - 1</sup>(0.7).</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-13.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>If the probability of a success in one trial is p and the probability of a failure is 1 - p, then the probability of finding the first success in the n<sup>th</sup> trial is given by<img src="../img/statistics-for-data-science/distribution-of-random-variables-14.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>The mean (i.e. expected value), variance, and standard deviation of this wait time are given by<img src="../img/statistics-for-data-science/distribution-of-random-variables-15.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>It takes, on average, 1/p trials to get a success under the geometric distribution. This mathematical result is consistent with what we would expect intuitively. If the probability of a success is high (e.g. 0.8), then we don't usually wait very long for a success: 1/0.8 = 1.25 trials on average. If the probability of a success is low (e.g. 0.1), then we would expect to view many trials before we see a success: 1/0.1 = 10 trials.</p>
                    <p><em>Example 8</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-16.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Binomial distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">binomial distribution</span> is used to describe the number of successes in a fixed number of trials. This is different from the geometric distribution, which described the number of trials we must wait before we observe a success.</p>
                    <p>Suppose the insurance agency is considering a random sample of four individuals they insure. What is the chance exactly one of them will exceed the deductible and the other three will not? Let's call the four people Ariana (A), Brittany (B), Carlton (C), and Damian (D) for convenience. P(A = exceed, B = not, C = not, D = not) = P(A = exceed) P(B = not) P(C = not) P(D = not) = (0.3)(0.7)(0.7)(0.7) = (0.7)<sup>3</sup>(0.3)<sup>1</sup> = 0.103. But there are three other scenarios: Brittany, Carlton, or Damian could have been the one to exceed the deductible. In each of these cases, the probability is again (0.7)<sup>3</sup>(0.3)<sup>1</sup>. These four scenarios exhaust all the possible ways that exactly one of these four people could have exceeded the deductible, so the total probability is 4 &times; (0.7)<sup>3</sup>(0.3)<sup>1</sup> = 0.412. The binomial distribution describes the probability of having exactly k successes in n independent Bernoulli trials with probability of a success p (n = 4, k = 3, p = 0.7).</p>
                    <p>Suppose the probability of a single trial being a success is p. Then the probability of observing exactly k successes in n independent trials is given by<img src="../img/statistics-for-data-science/distribution-of-random-variables-17.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>The mean, variance, and standard deviation of the number of observed successes are<img src="../img/statistics-for-data-science/distribution-of-random-variables-18.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>Is it binomial?</p>
                    <ol>
                        <li>The trials are independent.</li>
                        <li>The number of trials, n, is fixed.</li>
                        <li>Each trial outcome can be classified as a success or failure.</li>
                        <li>The probability of a success, p, is the same for each trial.</li>
                    </ol>
                    <p><em>Example 9</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-19.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Poisson distribution</h2>
                    <p>There are about 8 million individuals in New York City. How many individuals might we expect to be hospitalized for acute myocardial infarction (AMI), i.e. a heart attack, each day? According to historical records, the average number is about 4.4 individuals. However, we would also like to know the approximate distribution of counts. What would a histogram of the number of AMI occurrences each day look like if we recorded the daily counts over an entire year?</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-20.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>The sample mean (4.38) is similar to the historical average of 4.4. The sample standard deviation is about 2, and the histogram indicates that about 70% of the data fall between 2.4 and 6.4. The distribution's shape is unimodal and skewed to the right.</p>
                    <p>The <span class="badge rounded-pill bg-secondary">Poisson distribution</span> is often useful for estimating the number of events in a large population over a unit of time (the time unit is a day, the population is all New York City residents, and the historical rate is 4.4).</p>
                    <p>Suppose we are watching for events and the number of observed events follows a Poisson distribution with rate &lambda;. Then<img src="../img/statistics-for-data-science/distribution-of-random-variables-21.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 10</em>: A very skilled court stenographer makes one typographical error (typo) per hour on average. What probability distribution is most appropriate for calculating the probability of a given number of typos this stenographer makes in an hour? What are the mean and the standard deviation of the number of typos this stenographer makes? Would it be considered unusual if this stenographer made 4 typos in a given hour? Calculate the probability that this stenographer makes at most 2 typos in a given hour.<img src="../img/statistics-for-data-science/distribution-of-random-variables-22.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-23.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 11</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-24.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-25.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-26.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-27.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Probability density functions (PDFs) for a continuous random variable</h2>
                    <p>In the case of a continuous random variable, the probability of any individual outcome is theoretically zero because a continuous random variable is one that can assume an uncountable or infinite number of values. As such, we cannot list the possible values because there is an infinite number of them. However, we can determine the probability of a range of values.</p>
                    <p>The graph of a continuous probability distribution is a curve, and probability is represented by the area under the curve. The curve is called the <span class="badge rounded-pill bg-secondary">probability density function</span> (PDF), and we use the symbol &fnof;(x) to represent it. The following requirements apply to a probability density function &fnof;(x) whose range is a &le; x &le; b:</p>
                    <ol>
                        <li>the domain of &fnof; must be the set of all possible states of x</li>
                        <li>&fnof;(x) &ge; 0 for all x</li>
                        <li>the total area under the curve is 1</li>
                        <li>the probability that x has a value within the range a &le; x &le; b  is the area under the curve between x = a and x = b</li>
                    </ol>
                    <p>A function which describes the area under the curve is called a <span class="badge rounded-pill bg-secondary">cumulative distribution function</span> (CDF). The cumulative distribution function is used to evaluate probability as area. CDFs have the following properties:</p>
                    <ol>
                        <li>the outcomes are measured, not counted</li>
                        <li>the entire area under the curve and above the x-axis is equal to 1</li>
                        <li>probability is found for intervals (ranges) of x values rather than for individual x values</li>
                        <li>P(a &lt; x &lt; b) is the probability that the random variable X is in the interval between the values a and b; P(a &lt; x &lt; b) is the area under the curve, above the x-axis, to the right of a and the left of b</li>
                        <li>the probability that x takes on any single individual value is zero (P(x = a) = 0); the area below the curve, above the x-axis, and between x = a and x = a has no width, and therefore no area (area = 0); since the probability is equal to the area, the probability is also zero</li>
                        <li>P(a &lt; x &lt; b) is the same as P(a &le; x &le; b) since probability is equal to area</li>
                    </ol>
                    <p><em>Example 11</em>: Consider the function &fnof;(x) = 1/20 for 0 &le; x &le; 20. x = a real number. The graph of &fnof;(x) = 1/20 is a horizontal line. However, since 0 &le; x &le; 20, &fnof;(x) is restricted to the portion between x = 0 and x = 20, inclusive. The area between &fnof;(x) = 1/20 where 0 &le; x &le; 20 and the x-axis is the area of a rectangle with <span class="badge rounded-pill bg-secondary">base</span> = 20 and <span class="badge rounded-pill bg-secondary">height</span> = 1/20: <span class="badge rounded-pill bg-secondary">AREA = 20 &times; 1/20 = 1</span><img src="../img/statistics-for-data-science/distribution-of-random-variables-28.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">Suppose we want to find the area between &fnof;(x) = 1/20 and the x-axis where 0 &lt; x &lt; 2. <span class="badge rounded-pill bg-secondary">AREA = (2 – 0) &times; 1/20 = 0.1</span> (area of a rectangle = base &times; height) The area corresponds to a probability. The probability that x is between zero and two is 0.1, which can be written mathematically as P(0 &lt; x &lt; 2) = P(x &lt; 2) = 0.1.<img src="../img/statistics-for-data-science/distribution-of-random-variables-29.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">Suppose we want to find the area between &fnof;(x) = 1/20 and the x-axis where 4 &lt; x &lt; 15. <span class="badge rounded-pill bg-secondary">AREA = (15 – 4) &times; 1/20 = 0.55</span> The area corresponds to the probability P(4 &lt; x &lt; 15) = 0.55.<img src="../img/statistics-for-data-science/distribution-of-random-variables-30.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">Suppose we want to find P(x = 15). On an x-y graph, x = 15 is a vertical line. A vertical line has no width (or zero width). Therefore, P(x = 15) = base &times; height = 0 &times; 1/20 = 0.<img src="../img/statistics-for-data-science/distribution-of-random-variables-31.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Uniform distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">uniform distribution</span> is a continuous probability distribution and is concerned with events that are equally likely to occur. When working out problems that have a uniform distribution, be careful to note if the data is inclusive or exclusive of endpoints.</p>
                    <p><em>Example 12</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-32.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p><em>Example 13</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-33.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">Normal distribution</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">normal distribution</span> has two parameters (two numerical descriptive measures): the mean (&mu;) and the standard deviation (&sigma;). If X is a quantity to be measured that has a normal distribution with mean (&mu;) and standard deviation (&sigma;), we designate this by writing<img src="../img/statistics-for-data-science/distribution-of-random-variables-34.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>In theory, the mean is the same as the median, because the graph is symmetric about &mu;. As the notation indicates, the normal distribution depends only on the mean and the standard deviation. Since the area under the curve must equal one, a change in the standard deviation, &sigma;, causes a change in the shape of the curve; the curve becomes fatter or skinnier depending on &sigma;. A change in &mu; causes the graph to shift to the left or right. This means there are an infinite number of normal probability distributions. One of special interest is called the <span class="badge rounded-pill bg-secondary">standard normal distribution</span>.</p>
                    <p>The standard normal distribution is a normal distribution of standardized values called z-scores. A <span class="badge rounded-pill bg-secondary">z-score</span> is measured in units of the standard deviation. For example, if the mean of a normal distribution is five and the standard deviation is two, the value 11 is three standard deviations above (or to the right of) the mean. The calculation is as follows: x = &mu; + z &times; &sigma; = 5 + 3 &times; 2 = 11. The z-score is three.</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-35.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-36.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p><span class="badge rounded-pill bg-secondary">The z-score tells you how many standard deviations the value x is above (to the right of) or below (to the left of) the mean, &mu;.</span> Values of x that are larger than the mean have positive z-scores, and values of x that are smaller than the mean have negative z-scores. If x equals the mean, then x has a z-score of zero.</p>
                    <p><em>Example 14</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-37.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <p>If X is a random variable and has a normal distribution with mean &mu; and standard deviation &sigma;, then the <span class="badge rounded-pill bg-secondary">Empirical Rule</span> states the following:</p>
                    <ul>
                        <li>About 68% of the x values lie between –1σ and +1σ of the mean &mu; (within one standard deviation of the mean).</li>
                        <li>About 95% of the x values lie between –2σ and +2σ of the mean &mu; (within two standard deviations of the mean).</li>
                        <li>About 99.7% of the x values lie between –3σ and +3σ of the mean &mu; (within three standard deviations of the mean). Notice that almost all the x values lie within three standard deviations of the mean.</li>
                        <li>The z-scores for +1σ and –1σ are +1 and –1, respectively.</li>
                        <li>The z-scores for +2σ and –2σ are +2 and –2, respectively.</li>
                        <li>The z-scores for +3σ and –3σ are +3 and –3 respectively.</li>
                        <li>The empirical rule is also known as the 68-95-99.7 rule.</li>
                    </ul>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-38.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>The shaded area in the following graph indicates the area to the left of x. This area is represented by the probability P(X &lt; x). Normal tables, computers, and calculators provide or calculate the probability P(X &lt; x).</p>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-39.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p>The area to the right is then P(X &gt; x) = 1 – P(X &lt; x). Remember, P(X &lt; x) = <span class="badge rounded-pill bg-secondary">Area to the left</span> of the vertical line through x. P(X &gt; x) = 1 – P(X &lt; x) = <span class="badge rounded-pill bg-secondary">Area to the right</span> of the vertical line through x. P(X &lt; x) is the same as P(X &le; x) and P(X &gt; x) is the same as P(X &ge; x) for continuous distributions.</p>
                    <p><em>Example 15</em>: Normal distribution using Python</p>
                    <div class="row">
                        <div class="col-md"><img src="../img/statistics-for-data-science/distribution-of-random-variables-40.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></div>
                        <div class="col-md"><img src="../img/statistics-for-data-science/distribution-of-random-variables-41.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></div>
                    </div>
                    <p><em>Example 16</em>: Probability calculation using the normal distribution<img src="../img/statistics-for-data-science/distribution-of-random-variables-42.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-43.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-44.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                    <h2 class="h4">The central limit theorem</h2>
                    <p>The <span class="badge rounded-pill bg-secondary">central limit theorem</span> (clt for short) is one of the most powerful and useful ideas in all of statistics. There are two alternative forms of the theorem, and both alternatives are concerned with drawing finite samples size n from a population with a known mean, &mu;, and a known standard deviation, &sigma;. The first alternative says that if we collect samples of size n with a "large enough n," calculate each sample's mean, and create a histogram of those means, then the resulting histogram will tend to have an approximate normal bell shape. The second alternative says that if we again collect samples of size n that are "large enough," calculate the sum of each sample and create a histogram, then the resulting histogram will again tend to have a normal bell-shape.</p>
                    <p>The size of the sample, n, that is required in order to be "large enough" depends on the original population from which the samples are drawn (the sample size should be at least 30 or the data should come from a normal distribution). If the original population is far from normal, then more observations are needed for the sample means or sums to be normal. Sampling is done with replacement.</p>
                    <h3 class="h5">The central limit theorem for sample means (averages)</h3>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-45.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-46.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <h3 class="h5">The central limit theorem for sums</h3>
                    <img src="../img/statistics-for-data-science/distribution-of-random-variables-47.PNG" class="mx-auto d-block" alt="Distribution of Random Variables">
                    <p><em>Example 17</em>:<img src="../img/statistics-for-data-science/distribution-of-random-variables-48.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-49.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-50.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-51.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-52.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"><img src="../img/statistics-for-data-science/distribution-of-random-variables-53.PNG" class="mx-auto d-block" alt="Distribution of Random Variables"></p>
                </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingFour">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFour" aria-expanded="true" aria-controls="collapseFour">Introduction to Statistical Inference</button>
            </h2>
            <div id="collapseFour" class="accordion-collapse collapse" aria-labelledby="headingFour" data-bs-parent="#accordionExample">
                <div class="accordion-body"> <span class="badge rounded-pill bg-secondary"></span> <img src="../img/statistics-for-data-science/introduction-to-statistical-inference-1.PNG" class="mx-auto d-block" alt="Introduction to Statistical Inference"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingFive">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFive" aria-expanded="true" aria-controls="collapseFive">Hypothesis Testing</button>
            </h2>
            <div id="collapseFive" class="accordion-collapse collapse" aria-labelledby="headingFive" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingSix">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSix" aria-expanded="true" aria-controls="collapseSix">ANOVA, Goodness of Fit, and Bootstrapping</button>
            </h2>
            <div id="collapseSix" class="accordion-collapse collapse" aria-labelledby="headingSix" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingSeven">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSeven" aria-expanded="true" aria-controls="collapseSeven">Linear Regression</button>
            </h2>
            <div id="collapseSeven" class="accordion-collapse collapse" aria-labelledby="headingSeven" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingEight">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseEight" aria-expanded="true" aria-controls="collapseEight">Logistic Regression</button>
            </h2>
            <div id="collapseEight" class="accordion-collapse collapse" aria-labelledby="headingEight" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingNine">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseNine" aria-expanded="true" aria-controls="collapseNine">Time Series</button>
            </h2>
            <div id="collapseNine" class="accordion-collapse collapse" aria-labelledby="headingNine" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingTen">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTen" aria-expanded="true" aria-controls="collapseTen">Introduction to Causal Inference Part 1</button>
            </h2>
            <div id="collapseTen" class="accordion-collapse collapse" aria-labelledby="headingTen" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
        <div class="accordion-item">
            <h2 class="accordion-header" id="headingEleven">
                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseEleven" aria-expanded="true" aria-controls="collapseEleven">Introduction to Causal Inference Part 2</button>
            </h2>
            <div id="collapseEleven" class="accordion-collapse collapse" aria-labelledby="headingEleven" data-bs-parent="#accordionExample">
                <div class="accordion-body"> </div>
            </div>
        </div>
    </div>
</div>
<script src="../js/"></script> 
<script src="../js/bootstrap.min.js"></script>
</body>
</html>